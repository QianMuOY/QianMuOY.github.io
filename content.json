{"meta":{"title":"qianmu's blog","subtitle":"","description":"","author":"qianmu","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2023-03-20T05:14:50.000Z","updated":"2023-03-20T05:15:55.058Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-03-20T05:14:26.000Z","updated":"2023-03-20T05:16:34.634Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"link","date":"2023-03-20T05:49:33.000Z","updated":"2023-03-20T05:50:00.871Z","comments":true,"path":"link/index.html","permalink":"http://example.com/link/index.html","excerpt":"","text":""}],"posts":[{"title":"matplotlib","slug":"matplotlib","date":"2023-04-01T09:44:40.000Z","updated":"2023-04-01T09:49:21.989Z","comments":true,"path":"2023/04/01/matplotlib/","link":"","permalink":"http://example.com/2023/04/01/matplotlib/","excerpt":"","text":"matplotlibwhy matplotlib?如果您使用过“工科神器”MATLAB，您一定会惊叹于MATLAB中惊人的可视化能力，其中的函数绘图，成为了无数SCI论文中真实数据配图的来源。小小的plot函数，画出了人类先进科学进步的曲线。 python中的matplotlib库，与MATLAB中的plot函数有点像，也许这正是它名字的来源。这是一个2D绘图库，利用它可以画出许多高质量的图像。只需几行代码即可生成直方图，条形图，饼图，散点图等。Matplotlib可用于Python脚本，Python和IPython shell，Jupyter笔记本，Web应用程序服务器和四个图形用户界面工具包。 希望以下教程可以为您带来一些帮助。 安装请查看上方anaconda教程，配置好python环境。使用命令pip install matplotlib 或 conda install matplotlib 进行安装。使用时，直接import即可。 Matplotlib.pyplotMatplotlib 中的 pyplot 模块是一个类似命令风格的函数集合，这使得 Matplotlib 的工作模式和 MATLAB 相似。 pyplot 模块提供了可以用来绘图的各种函数，比如创建一个画布，在画布中创建一个绘图区域，或是在绘图区域添加一些线、标签等。以下表格对这些函数做了简单地介绍。若您有MATLAB基础，看着本节即可上手使用。 绘图类型 函数名称 描述 Bar 绘制条形图 Barh 绘制水平条形图 Boxplot 绘制箱型图 Hist 绘制直方图 his2d 绘制2D直方图 Pie 绘制饼状图 Plot 在坐标轴上画线或者标记 Polar 绘制极坐标图 Scatter 绘制x与y的散点图 Stackplot 绘制堆叠图 Stem 用来绘制二维离散数据绘制（又称为“火柴图”） Step 绘制阶梯图 Quiver 绘制一个二维按箭头 Image函数 函数名称 描述 Imread 从文件中读取图像的数据并形成数组。 Imsave 将数组另存为图像文件。 Imshow 在数轴区域内显示图像。 Axis函数 函数名称 描述 Axes 在画布(Figure)中添加轴 Text 向轴添加文本 Title 设置当前轴的标题 Xlabel 设置x轴标签 Xlim 获取或者设置x轴区间大小 Xscale 设置x轴缩放比例 Xticks 获取或设置x轴刻标和相应标签 Ylabel 设置y轴的标签 Ylim 获取或设置y轴的区间大小 Yscale 设置y轴的缩放比例 Yticks 获取或设置y轴的刻标和相应标签 Figure函数 函数名称 描述 Figtext 在画布上添加文本 Figure 创建一个新画布 Show 显示数字 Savefig 保存当前画布 Close 关闭画布窗口 第一个绘图程序首先导入 Matplotlib 包中的 Pyplot 模块，并以 as 别名的形式简化引入包的名称。 1import matplotlib.pyplot as plt 接下来，使用 NumPy 提供的函数 arange() 创建一组数据来绘制图像。 如果您对numpy库并不了解，可以理解为这就是在生成列表数据即可。 1234#引入numpy包import numpy as np#获得0到2π之间的数据，每个数据间隔0.05x = np.arange(0, math.pi*2, 0.05) 我们这里来绘制sin函数的值。使用numpy库内置函数计算出值（实际上是矩阵），放入y中，作为函数值。 1y = np.sin(x) 此时，使用plot函数进行绘制即可。绘制完后，需要使用show函数展示出来才行。 12plt.plot(x,y)plt.show() 这个图像实在是光秃秃。我们在show之前添加一些属性设置。设置上图像的标题、标签。 123plt.xlabel(&quot;angle&quot;)plt.ylabel(&quot;sine&quot;)plt.title(&#x27;sine wave&#x27;) 这样，图像勉强能看了。 您也可以在 Jupyter 笔记本中运行 Matplotlib 的绘图程序。通过命令行或者开始菜单的方式启动 Jupyter 笔记本。启动成功后，将上述代码拷贝到输入行内，如下所示： %matplotlib inline 是 Jupyter 提供的魔法命令，它可以把输出图显示在笔记本内部，否则会以查看器的形式单独显示。 title、label、坐标轴设置123456plt.title(&quot;title&quot;)#括号当中输入标题的名称plt.xlim(0,6) #x轴坐标轴plt.ylim((0, 3))#y轴坐标轴plt.xlabel(&#x27;X&#x27;)#x轴标签plt.ylabel(&#x27;Y&#x27;)#y轴标签plt.show() 可以对相应属性进行设置。 其中，x、y坐标轴的设置，又一般使用plot函数进行设置。 Figure画布对象与Axes坐标轴对象在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个或者多个Axes对象。每个Axes(ax)对象都是一个拥有自己坐标系统的绘图区域。 1234plt.figure(figsize=(6, 3))plt.plot(6, 3)plt.plot(3, 3 * 2)plt.show() 如果我们要在一个代码中生成多张图，就需要多个画布，也就是多个figure对象。那么此时，我们就转变为对各个画布对象进行操作，而非plt库的默认画布。 123from matplotlib import pyplot as plt#创建图形对象fig = plt.figure() 参数 说明 figsize 指定画布的大小，(宽度,高度)，单位为英寸。 dpi 指定绘图对象的分辨率，即每英寸多少个像素，默认值为80。 facecolor 背景颜色。 dgecolor 边框颜色。 frameon 是否显示边框。 有了画布，我们还要添加坐标轴对象。 1ax = fig.add_axes([0.1,0.1,0.8,0.8]) add_axes() 的参数值是一个序列，序列中的 4 个数字分别对应图形的左侧，底部，宽度，和高度，且每个数字必须介于 0 到 1 之间。 即将画布的宽、高作为 1 个单位。比如，[ 0.1, 0.1, 0.8, 0.8]，它代表着从画布 10% 的位置开始绘制, 宽高是画布的 80%。 坐标轴对象可以设置标题，设置标签，进行绘图。与原本的库一样的调用方式。 123456789x = np.arange(0, math.pi*2, 0.05)y = np.sin(x)fig = plt.figure()ax = fig.add_axes([0.1,0.1,0.8,0.8])ax.plot(x,y)ax.set_title(&quot;sine wave&quot;)ax.set_xlabel(&#x27;angle&#x27;)ax.set_ylabel(&#x27;sine&#x27;)plt.show() label、legend图例设置在画图时，可以添加label属性，传入图例。并通过legend启动，添加loc属性选择图例位置。 12345x = np.arange(0, math.pi*2, 0.05)y = np.sin(x)plt.plot(x,y,label=&quot;sin&quot;)plt.legend(loc=&quot;best&quot;)#图例位置，可选best，center等plt.show() 一图多线在同一个figure下，直接使用plot进行画图即可。同一张图上可以全部显示出来。注意添加label参数以作区别。 如果要画多张图，可以在两个plot方法之间使用figure方法创造一个新的窗口，进行分别展示。 注释有时候我们需要对特定的点进行标注，我们可以使用 plt.annotate 函数来实现: s: 注释信息内容 xy:箭头点所在的坐标位置 xytext:注释内容的坐标位置 arrowprops：设置指向箭头的参数 123456x=np.linspace(0,10,200)#从0到10之间等距产生200个值y=np.sin(x)plt.plot(x,y,linestyle=&#x27;:&#x27;,color=&#x27;b&#x27;)plt.annotate(text=&#x27;here&#x27;,xy=(3,np.sin(3)),xytext=(4,-0.5),weight=&#x27;bold&#x27;,color=&#x27;b&#x27;,arrowprops=dict(arrowstyle=&#x27;-|&gt;&#x27;,color=&#x27;k&#x27;))plt.show() 子图如果需要将多张子图展示在一起，可以使用 subplot() 实现。即在调用 plot()函数之前需要先调用 subplot() 函数。 该函数的第一个参数代表子图的总行数， 第二个参数代表子图的总列数， 第三个参数代表活跃区域。 1234567891011ax1 = plt.subplot(2, 2, 1)plt.plot(x,np.sin(x), &#x27;k&#x27;)ax2 = plt.subplot(2, 2, 2, sharey=ax1) # 与 ax1 共享y轴plt.plot(x, np.cos(x), &#x27;g&#x27;)ax3 = plt.subplot(2, 2, 3)plt.plot(x,x, &#x27;r&#x27;)ax4 = plt.subplot(2, 2, 4, sharey=ax3) # 与 ax3 共享y轴plt.plot(x, 2*x, &#x27;y&#x27;) 同样的，这些图像支持各自传入各种各样的参数。 plot的参数plot函数支持传入各种参数，不需要在外部再手动设置。 1234#单条线：plot([x], y, [fmt], data=None, **kwargs)#多条线一起画plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs) 可选参数[fmt] 是一个字符串来定义图的基本属性如：颜色（color），点型（marker），线型（linestyle）， 具体形式 fmt &#x3D; ‘[color] [ marker ] [ line ]’ fmt接收的是每个属性的单个字母缩写。 ============= =============================== character color ============= =============================== ``&#39;b&#39;`` blue 蓝 ``&#39;g&#39;`` green 绿 ``&#39;r&#39;`` red 红 ``&#39;c&#39;`` cyan 蓝绿 ``&#39;m&#39;`` magenta 洋红 ``&#39;y&#39;`` yellow 黄 ``&#39;k&#39;`` black 黑 ``&#39;w&#39;`` white 白 ============= =============================== 1234567891011121314151617181920212223242526============= =============================== character description ============= =============================== ``&#x27;.&#x27;`` point marker ``&#x27;,&#x27;`` pixel marker ``&#x27;o&#x27;`` circle marker ``&#x27;v&#x27;`` triangle_down marker ``&#x27;^&#x27;`` triangle_up marker ``&#x27;&lt;&#x27;`` triangle_left marker ``&#x27;&gt;&#x27;`` triangle_right marker ``&#x27;1&#x27;`` tri_down marker ``&#x27;2&#x27;`` tri_up marker ``&#x27;3&#x27;`` tri_left marker ``&#x27;4&#x27;`` tri_right marker ``&#x27;s&#x27;`` square marker ``&#x27;p&#x27;`` pentagon marker ``&#x27;*&#x27;`` star marker ``&#x27;h&#x27;`` hexagon1 marker ``&#x27;H&#x27;`` hexagon2 marker ``&#x27;+&#x27;`` plus marker ``&#x27;x&#x27;`` x marker ``&#x27;D&#x27;`` diamond marker ``&#x27;d&#x27;`` thin_diamond marker ``&#x27;|&#x27;`` vline marker ``&#x27;_&#x27;`` hline marker ============= =============================== 12345678============= ===============================character description============= ===============================``&#x27;-&#x27;`` solid line style 实线``&#x27;--&#x27;`` dashed line style 虚线``&#x27;-.&#x27;`` dash-dot line style 点画线``&#x27;:&#x27;`` dotted line style 点线============= =============================== 另外，支持color&#x3D;，linestyle&#x3D;，label&#x3D;关键字传参，具体可参照官方文档，或参考上方plt.xxx设置。一般都有对应的参数。 各种其他图参考绘图类型节即可。这里以柱状图为例。 1234x = np.arange(10)y = np.random.randint(0,20,10)plt.bar(x, y)plt.show()","categories":[],"tags":[]},{"title":"git","slug":"git","date":"2023-03-27T08:28:52.000Z","updated":"2023-03-27T08:29:07.787Z","comments":true,"path":"2023/03/27/git/","link":"","permalink":"http://example.com/2023/03/27/git/","excerpt":"","text":"git远程仓库网址：https://github.com 张朝阳账户：&#x7a;&#104;&#97;&#x6e;&#x67;&#x7a;&#104;&#97;&#111;&#x79;&#x61;&#110;&#x67;&#64;&#x63;&#x68;&#100;&#46;&#x65;&#100;&#x75;&#46;&#99;&#x6e; 用户名zhzj0218，密码：一般密码 以将&#x2F;d&#x2F;gitproject&#x2F;pycharm_python_and_pyqt中的文件同步到git中的zhzj0218下的pycharm_python_and_pyqt为例 [TOC] 查看分支状态git status 创建版本库git init 把当前的目录变成Git管理的版本库 git status 查看当前分支状态 git add . 把当前文件夹下的文件提交到暂存区 紧接着git commit -m “提交标签，如first proposed” 把暂存区提交到版本库 执行git status查看状态，确保工作区是干净的，没有需要提交的内容 （git log查看提交日志，git reflog查看提交版本号） 远程同步git remote -v查看远程版本库信息 创建github上的git仓库，例如在github的zhzj0218下创建了仓库pycharm_python_and_pyqt.git git remote add origin https://github.com/zhzj0218/pycharm_python_and_pyqt.git 将本地仓库关联到Github仓库 再次查看git remote -v，确认远程仓库关联成功 git push -u origin master 将本地仓库的内容推送到Github仓库 远程库克隆到本地git clone https://github.com/zhzj0218/pycharm_python_and_pyqt.git 将远程仓库的内容克隆到本地 常用命令创建版本库git clone 克隆远程版本库到本地 git init 初始化本地版本库 修改和提交git status 查看状态，也可查看冲突的文件 git dif 查看变更内容 git add . 跟踪所有改动过的内容 git add 跟踪制定的文件 git mv 文件改名 git rm 删除文件 git rm –cached 停止跟踪文件但不删除 git commit -m “commit message” 提交所有更新过的文件 git commit –amend 修改最后一次提交 查看提交历史git log 查看提交历史 git log -p 查看指定文件的提交历史 git blame 以列表方式查看指定文件的提交历史 撤销git reset –hard HEAD 撤销工作目录中所有未提交文件的修改内容 git checkout HEAD 撤销指定的未提交文件的修改内容 git revert 撤销指定的提交 git reset –hard HEAD^ 还原到上一个版本 git reset –hard 18b6a 回到之前版本，此处18b6a为某个版本号，版本号无需写全 （git log查看提交历史，可看到版本号，git log –pretty&#x3D;oneline可简化输出查看版本号，HEAD^表示上一次版本，HEAD^^表示上上一个版本，也可以用数字表示，HEAD~2，也可以用git reflog查看版本号，此时的版本号较短，为简短版本号） git checkout – 文件没有添加到暂存区时，撤回工作区的修改 git reset HEAD 文件已经添加到暂存区时，撤销暂存区的修改，然后再使用git check – 撤回工作区的修改 分支与标签git branch 显示所有本地分支 git checkout &lt;branch&#x2F;tag&gt; 切换到指定分支或者标签 git branch 创建新分支 git branch -d 删除本地分支 git tag 列出所有本地标签 git tag 基于最新提交创建标签 git tag -d 删除标签 合并与衍合git merge 合并指定分支到当前分支 git rebase 衍合指定分支到当前分支 远程操作git remote -v 查看远程版本库信息 git remote show 查看指定远程版本库信息 git remote add 添加远程版本库 git fetch 从远程库获取代码 git pull 下载代码及快速合并 git push 上传代码及快速合并 git push :&lt;branch&#x2F;tag-name&gt; 删除远程分支或标签 git push –tags 上传所有标签 git remote rm origin移除原先错误的origin remote 查看和修改用户名和邮箱git config user.name查看用户名 git config user.email查看邮箱 git config –global user.name “your name”修改用户名为your name git config –global user.email “your email”修改用户名为your email 密码输入错误的解决方法在控制面板，管理windows凭证，修改正确的密码 大文件上传git默认不能上传超过100M的文件，如果上传100M的文件，则会出现fatal: The remote end hung up unexpectedly的错误，表示存在大文件不能上传。因此需要处理大文件的上传。 示例：假设当前存在后缀为.exe，.zip及.whl的大文件，处理步骤如下： git lfs install 安装lfs git lfs track “*.zip” git lfs track “*.exe” git lfs track “*.whl” 则文件夹中生成的.gitattributes文件内容如下： 123*.exe filter=lfs diff=lfs merge=lfs -text*.zip filter=lfs diff=lfs merge=lfs -text*.whl filter=lfs diff=lfs merge=lfs -text git add .gitattributes 将.gitattributes文件加入到暂存区 git commit -m “add lfs file” 提交文件 git push origin master 将本地仓库内容提交到Github仓库（包含大文件提交，提交的大文件具有lfs标签，表明为大文件）","categories":[],"tags":[]},{"title":"基于深度学习的端到端自动驾驶","slug":"基于深度学习的端到端自动驾驶","date":"2023-03-27T08:24:47.000Z","updated":"2023-03-27T08:25:05.435Z","comments":true,"path":"2023/03/27/基于深度学习的端到端自动驾驶/","link":"","permalink":"http://example.com/2023/03/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/","excerpt":"","text":"基于深度学习的端到端自动驾驶from:欧阳铭 email: &#99;&#x68;&#x64;&#x2d;&#113;&#x69;&#97;&#110;&#109;&#x75;&#64;&#x71;&#113;&#46;&#99;&#111;&#x6d; qq:3315853846 特别感谢:钱斌的博客 ##1.模拟平台安装和基本使用 ###1.1模拟器的安装 下载地址：右键或点击此处或按住ctrl+&#x2F;进入代码模式，复制链接到浏览器打开。 该地址中提供的模拟器是基于Unity开发的，是经过删减过后的可执行程序，不再需要额外安装unity，下载下来后就可以直接运行。目前覆盖windows、Linux、Mac共3个版本。以下只讲解如何在windows平台上运行和使用该模拟器。 下载完成后运行donkey_sim.exe文件即可进入模拟器。左边是设置，根据自己的需要进行设置即可。 在场景中，如果我们前面主界面使用了手工模式（paceCar处勾选manualDriving），那么我们就可以通过键盘来操控小车进行体验了。与一般的赛车游戏类似，W键表示前进，A表示左转，D表示右转，S表示后退。 在该模拟器中，控制小车的主要是两个参数：油门（W和S键）和转向角度（A和D键），这个与我们真实驾驶的汽车基本一致：挂挡+踩油门来控制前进动力，打方向盘控制车辆转向。为了能够实现自动驾驶，我们首先要能够根据这两个参数去控制模拟器里面小车的运行。我们怎么样通过Python代码来控制这个模拟器呢？ 1.2git工具的安装与配置详细的git工具安装与教程请参照这篇文章，这里仅描述如何在pycharm中使用git 1.2.1注册github账号 GitHub官网 按照提示进行注册。请注意记下用户名和邮箱地址，下面步骤需要使用。 ####1.2.2安装git 到git官网下载git安装包，点击downloads，选择操作平台(windows)，下载完成后运行，第一次选择全勾，后面一直下一步即可。 下载完成后，运行cmd，输入命令检查下载版本。能正确显示版本即为安装成功 1git version ####1.2.3配置git 安装成功后，配置git 123git config --global user.name &quot;用户名&quot;git config --global user.email &quot;用户邮箱&quot; 请注意此处空格必须严格遵循，否则将无法配置成功。建议直接复制过去改。 检查配置是否成功。在最下面会出现属性user.name与user.email，如果没有请重新到回到上一步。 1git config --list 1.2.4在pycharm中配置gitfile-&gt;settings-&gt;version control-&gt;git，一般此处默认检测系统中安装的第一个git，如果没有检测到请手动选择。 具体git操作请参照这篇文章 这里不进行描述。 ###1.3自动驾驶初体验 这个模拟器的好处就在于预留了Python控制接口，我们只需要安装一个驱动库就可以直接驱动模拟器里面的小车运行（提前安装好Git工具）： 在pycharm下方终端输入以下pip命令 1pip install git+https://github.com/tawnkramer/gym-donkeycar 安装好以后我们可以运行下面的python代码来实现小车的控制（注意：运行下面的代码前先启动模拟器，并停留在模拟器对应地图里） ： 1234567891011121314151617181920212223242526# 导入库import gymimport gym_donkeycarimport numpy as npimport cv2 # 设置模拟器环境env = gym.make(&quot;donkey-generated-roads-v0&quot;) # 重置当前场景obv = env.reset() # 运行100帧for t in range(100): # 定义控制动作 action = np.array([0.3,0.5]) # 动作控制，0.3表示转向，0.5表示油门 # 执行动作 obv, reward, done, info = env.step(action) # 取一张图像保存 if t == 20: img = cv2.cvtColor(obv,cv2.COLOR_RGB2BGR) cv2.imwrite(&#x27;test.jpg&#x27;,img) # 运行完以后重置当前场景obv = env.reset() 我们先分析下这段代码。下面这行代码用于设置模拟器环境，简单来说就是启用哪张地图： 1env = gym.make(&quot;donkey-generated-roads-v0&quot;) 在这个模拟器里面我们可以用到的地图如下所示： “donkey-warehouse-v0” “donkey-generated-roads-v0” “donkey-avc-sparkfun-v0” “donkey-generated-track-v0” “donkey-roboracingleague-track-v0” “donkey-waveshare-v0” “donkey-minimonaco-track-v0” “donkey-warren-track-v0” “donkey-thunderhill-track-v0” “donkey-circuit-launch-track-v0” 接下来的代码里面，我们运行了100帧，每帧都用固定的控制参数来执行：右转0.3、前进0.5。这两个字段就是我们前面提到的转向和油门值。下面给出这两个值的具体定义： 油门值取值范围是[-1，1]，负值代表倒退，正值代表前进。转向值取值范围也是[-1，1]，负值代表向左，正值代表向右。 接下来使用np.array封装这两个参数，然后通过env.step来执行单步动作。执行完动作以后会返回一些信息，其中我们需要重点关注obs这个返回参数，这个参数表示当前位于小车正中间行车记录仪摄像头返回的一帧图像 ，图像宽160像素，高120像素，3通道RGB图像。可以在代码根目录下找到test.jpg文件查看。 numpy教程十分钟入门 openCV教程 黑马程序员 建议观看以上视频，学习一定的基础知识。当然没有这样的知识硬记语法也是可以的。 2.基于OpenCV的自动驾驶控制在学习自动驾驶前，我们先看看传统算法是怎么解决自动驾驶任务的。本节为后续自动驾驶作铺垫，如果不感兴趣可以直接跳转到深度学习部分。 我们希望通过算法来控制小车，最终让这个小车稳定运行在行车道内。这里面涉及到两方面：感知和动作规划。感知部分我们主要通过行道线检测来实现，动作规划通过操控转向角度来实现。行道线检测的目的就是希望能够根据检测到的行道线位置来计算最终应该转向的角度，从而控制小车始终运行在当前车道线内。 由于道路环境比较简单，针对我们这个任务，我们进一步简化我们的控制变量，我们只控制转向角度，对于油门值我们在运行时保持低匀速，这样我们的重点就可以放在一个变量上面—转向角度。 2.1基于HSV空间的特定颜色区域提取 颜色过滤是目前经常被使用到的图像处理技巧之一，例如天气预报抠像等，经常会使用绿幕作为背景进行抠图。本小节使用颜色过滤来初步提取出行道线。 从模拟平台的图像数据上进行分析，小车左侧是黄实线，右侧是白实线。我们希望小车一直运行在这两根线之间。因此，我们首先要定位出这两根线。我们可以通过颜色空间变换来定位这两根线。 为了方便将黄色线和白色线从图像中过滤出来，我们需要将图像从RGB空间转换到HSV空间再处理。 这里首先我们解释下RGB和HSV颜色空间的区别。 RGB 是我们接触最多的颜色空间，由三个通道表示一幅图像，分别为红色(R)，绿色(G)和蓝色(B)。这三种颜色的不同组合可以形成几乎所有的其他颜色。RGB 颜色空间是图像处理中最基本、最常用、面向硬件的颜色空间，比较容易理解。RGB 颜色空间利用三个颜色分量的线性组合来表示颜色，任何颜色都与这三个分量有关，而且这三个分量是高度相关的，所以连续变换颜色时并不直观，想对图像的颜色进行调整需要更改这三个分量才行。自然环境下获取的图像容易受自然光照、遮挡和阴影等情况的影响，即对亮度比较敏感。而 RGB 颜色空间的三个分量都与亮度密切相关，即只要亮度改变，三个分量都会随之相应地改变，而没有一种更直观的方式来表达。但是人眼对于这三种颜色分量的敏感程度是不一样的，在单色中，人眼对红色最不敏感，蓝色最敏感，所以 RGB 颜色空间是一种均匀性较差的颜色空间。如果颜色的相似性直接用欧氏距离来度量，其结果与人眼视觉会有较大的偏差。对于某一种颜色，我们很难推测出较为精确的三个分量数值来表示。所以，RGB 颜色空间适合于显示系统，却并不适合于图像处理。 基于上述理由，在图像处理中使用较多的是 HSV 颜色空间，它比 RGB 更接近人们对彩色的感知经验。非常直观地表达颜色的色调、鲜艳程度和明暗程度，方便进行颜色的对比。在 HSV 颜色空间下，比 BGR 更容易跟踪某种颜色的物体，常用于分割指定颜色的物体。HSV 表达彩色图像的方式由三个部分组成： Hue（色调、色相） Saturation（饱和度、色彩纯净度） Value（明度） 其中Hue用角度度量，取值范围为0～360°，表示色彩信息，即所处的光谱颜色的位置，如下图所示。 如果我们想要过滤出黄色线，那么我们就可以将色调范围控制在[30~ 90]之间即可。注意：在OpenCV中色调范围是[0~ 180]，因此上述黄色范围需要缩小1倍，即[15~ 45]。检测白色行道线也是采用类似的原理。 RGB转化到HSV的算法(c++): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051float retmax(float a,float b,float c)//求最大值&#123; float max = 0; max = a; if(max&lt;b) max = b; if(max&lt;c) max = c; return max;&#125;float retmin(float a,float b,float c)//求最小值&#123; float min = 0; min = a; if(min&gt;b) min = b; if(min&gt;c) min = c; return min;&#125;//R,G,B参数传入范围（0~100）//转换结果h(0~360),s(0~100),v(0~100)void rgb_to_hsv(float *h,float *s,float *v,float R,float G,float B)&#123; float max = 0,min = 0; R = R/100; G = G/100; B = B/100; max = retmax(R,G,B); min = retmin(R,G,B); *v = max; if(max == 0) *s = 0; else *s = 1 - (min/max); if(max == min) *h = 0; else if(max == R &amp;&amp; G&gt;=B) *h = 60*((G-B)/(max-min)); else if(max == R &amp;&amp; G&lt;B) *h = 60*((G-B)/(max-min)) + 360; else if(max == G) *h = 60*((B-R)/(max-min)) + 120; else if(max == B) *h = 60*((R-G)/(max-min)) + 240; *v = *v * 100; *s = *s * 100;&#125; HSV转RGB算法：(c++) 123456789101112131415161718192021222324252627282930313233//参数入参范围h(0~360),s(0~100),v(0~100),这里要注意，要把s,v缩放到0~1之间//转换结果R(0~100),G(0~100),B(0~100)，如需转换到0~255，只需把后面的乘100改成乘255void hsv_to_rgb(int h,int s,int v,float *R,float *G,float *B)&#123; float C = 0,X = 0,Y = 0,Z = 0; int i=0; float H=(float)(h); float S=(float)(s)/100.0; float V=(float)(v)/100.0; if(S == 0) *R = *G = *B = V; else &#123; H = H/60; i = (int)H; C = H - i; X = V * (1 - S); Y = V * (1 - S*C); Z = V * (1 - S*(1-C)); switch(i)&#123; case 0 : *R = V; *G = Z; *B = X; break; case 1 : *R = Y; *G = V; *B = X; break; case 2 : *R = X; *G = V; *B = Z; break; case 3 : *R = X; *G = Y; *B = V; break; case 4 : *R = Z; *G = X; *B = V; break; case 5 : *R = V; *G = X; *B = Y; break; &#125; &#125; *R = *R *100; *G = *G *100; *B = *B *100;&#125; 而以上方法在opencv中都有封装，仅需要一句代码即可实现。现在我们来检测黄色线与白色线。 代码实现： 123456789101112131415161718import cv2import numpy as np #读取图像并转换到HSV空间frame = cv2.imread(&#x27;test.jpg&#x27;)hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 黄色线检测lower_blue = np.array([15, 40, 40])upper_blue = np.array([45, 255, 255])yellow_mask = cv2.inRange(hsv, lower_blue, upper_blue)cv2.imwrite(&#x27;yellow_mask.jpg&#x27;,yellow_mask) # 白色线检测lower_blue = np.array([0, 0, 200])upper_blue = np.array([180, 30, 255])white_mask = cv2.inRange(hsv, lower_blue, upper_blue)cv2.imwrite(&#x27;white_mask.jpg&#x27;,white_mask) 运行效果如下图： 可以看到还有很大的干扰存在。主要来自于相似的颜色，因此我们还需要进行进一步的处理。 2.2基于canny算子的边缘轮廓提取 目前我们仅获得了行道线区域，为了后续能够方便的计算行道线角度，我们需要得到行道线具体的轮廓&#x2F;线段信息，即从区域中提取出线段。这里我们使用Canny算法实现。 Canny边缘检测是从不同视觉对象中提取有用的结构信息并大大减少要处理的数据量的一种技术，于1986年被提出，目前已广泛应用于各种计算机视觉系统。 Canny算法具体包括5个步骤： 使用高斯滤波器，以平滑图像，滤除噪声。 计算图像中每个像素点的梯度强度和方向。 应用非极大值（Non-Maximum Suppression）抑制，以消除边缘检测带来的杂散响应。 应用双阈值（Double-Threshold）检测来确定真实的和潜在的边缘。 通过抑制孤立的弱边缘最终完成边缘检测。 具体实现细节我们不再详细剖析，在OpenCV中集成了canny算法，只需要一行代码即可实现canny边缘检测。 1234567# 黄色线边缘提取yellow_edge = cv2.Canny(yellow_mask, 200, 400)cv2.imwrite(&#x27;yellow_edge.jpg&#x27;, yellow_edge) # 白色线边缘提取whitewhite_edge = cv2.Canny(white_mask, 200, 400)cv2.imwrite(&#x27;white_edge.jpg&#x27;, white_edge) 有兴趣了解canny的同学可以查看这篇博客 ,其中运用到的正态分布、偏导、梯度与极限的知识我们都学过，可以理解。高斯核滤波也用于卷积，而实际上的卷积还会复杂一点，可以自行了解。在后面的pytorch版本我会详细解释运行原理。 代码中200和400这两个参数表示canny算子的低、高阈值，按照opencv教程一般可以不用修改。 最终效果： 2.3感兴趣区域(ROI)定位在利用OpenCV对图像进行处理时，通常会遇到一个情况，就是只需要对部分感兴趣区域（Region Of Interest, ROI）进行处理。例如针对我们这个模拟平台上的智能小车任务来说，对于黄色行道线，我们只关注图像右下部分，而对于白色行道线，我们只关注图像左下部分即可。至于图像其他部分因为我们通过人工分析知道，这些区域我们并不需要处理。因此，我们只用提取图像的对应区域。 1234567891011121314151617181920212223242526272829303132import cv2import numpy as npdef region_of_interest(edges, color=&#x27;yellow&#x27;): &#x27;&#x27;&#x27; 感兴趣区域提取 &#x27;&#x27;&#x27; height, width, _ = edges.shape mask = np.zeros_like(edges) # 定义感兴趣区域掩码轮廓 if color == &#x27;yellow&#x27;: polygon = np.array([[(width * 1 / 2, height * 1 / 2), (width, height * 1 / 2), (width, height), (width * 1 / 2, height)]], np.int32) else: polygon = np.array([[(0, height * 1 / 2), (width * 1 / 2, height * 1 / 2), (width * 1 / 2, height), (0, height)]], np.int32) # 填充感兴趣区域掩码 cv2.fillPoly(mask, polygon, (255,255,255)) # 提取感兴趣区域 croped_edge = cv2.bitwise_and(edges, mask) return croped_edgeif __name__==&quot;__main__&quot;: yimg = cv2.imread(&#x27;yellow_edge.jpg&#x27;) wimg = cv2.imread(&#x27;white_edge.jpg&#x27;) cv2.imwrite(&#x27;white_edge_new.jpg&#x27;, region_of_interest(wimg, &#x27;white&#x27;)) cv2.imwrite(&#x27;yellow_edge_new.jpg&#x27;, region_of_interest(yimg)) 这里定义感兴趣区域、填充感兴趣区域是使用了暴力的直接定位法，按住ctrl+函数名可以进入cv2.fillPoly查看函数的参数与具体作用。在实际应用中我们还有一些更加高级的方法来锁定感兴趣区域，但对于模拟器而言这种方法已经够用。注意这一段代码： 12345&#x27;&#x27;&#x27; 感兴趣区域提取 &#x27;&#x27;&#x27; height, width, _ = edges.shape mask = np.zeros_like(edges) edges.shape返回有三个参数，分别为高度、宽度、通道数，如rgb图像就是三通道的。但是下面我们用不到通道数的值，但解包赋值需要把返回值全部接受，因此此处用_ 暂时存放通道数。 对于如何填充矩形有兴趣的同学，可以查看这篇博客 。这里进行部分的转载。 一、fillConvexPoly( )函数以填充矩形为例我图中想填充以1、2、3、4为顶点的矩形，我就要按1、2、3、4的顺序给出坐标序列array，或者是连贯的相邻顶点顺序（比如1432，4321…） 12rectangular = np.array([ [0,0],[0,740], [4032,740], [4032,0] ])cv2.fillConvexPoly(img_gray, rectangular, (0,0,0)) 效果如下图所示 那如果不按邻边顺序，如果我写成1423会如何呢？来看。 emmm， 我觉得想画五角星的画可以这样。 二、fillPoly( )函数好了，上面都是基本操作，看看官方教程都可以。我们玩点别的。我如果想把一个矩形区域留住（拿车牌举例），剩下的填充为白色应该怎么办呢？举一反三一下，两种办法：（1） 1cv2.fillPoly(img_gray, [rec1, rec2, rec3, rec4], (255,255,255)) 四个矩形，改用fillPoly（）方法。 结果如上图。 （2）非要死脑筋其实用fillConvexPoly( )也不是不可以。 画个示意图，你们懂我意思吧。 定位后，我们的图片如下所示，可以看到更加“整洁”了。 ###2.4基于霍夫变换的线段检测 到目前，我们抽取出了比较精确的行道线轮廓，但是对于实际的自动驾驶任务来说还没有完成目标任务要求，我们要对行道线轮廓再进一步处理，得到行道线的具体线段信息（每条线段的起始点坐标）。本小节我们使用霍夫变换来完成这个任务。霍夫变换，英文名称Hough Transform，作用是用来检测图像中的直线或者圆等几何图形的。 具体的，一条直线的表示方法有好多种，最常见的是y&#x3D;mx+b的形式。结合我们这个任务，对于最终检测出的感兴趣区域，怎么把图片中的直线提取出来。基本的思考流程是：如果直线 y&#x3D;mx+b 在图片中，那么图片中，必需有N多点在直线上（像素点代入表达式成立），只要有这条直线上的两个点，就能确定这条直线。该问题可以转换为：求解所有的(m,b)组合。【以下是部分原理，不感兴趣的同学可以直接跳到代码】 设置两个坐标系，左边的坐标系表示的是(x,y)值，右边的坐标系表达的是(m,b)的值，即直线的参数值。那么一个(x,y)点在右边对应的就是一条线，左边坐标系的一条直线就是右边坐标系中的一个点。这样，右边左边系中的交点就表示有多个点经过(k,b)确定的直线。但是，该方法存在一个问题，(m,b)的取值范围太大。 为了解决(m,b)取值范围过大的问题，在直线的表示方面用 xcosθ+ysinθ&#x3D;r 的规范式代替一般表达式，参数空间变成(θ,r)，0&#x3D;&lt;θ&lt;&#x3D;2PI。这样图像空间中的一个像素点在参数空间中就是一条曲线（三角函数曲线）。 此时，图像空间和参数空间的对应关系如下： 从图中可以看出，霍夫直线检测即为在参数空间中对r和theta投票的过程，得票最高者为最终的直线参数。 theta表示与直线垂直的线与x轴的夹角，那么他的取值范围就是-pi到pi ，但显然取0-pi就可以表示所有直线。 更详细的数学原理请查看这篇博客 ，其中涉及到笛卡尔坐标向极坐标变换(事实上是一个特殊的参数空间)的讨论，对数学感兴趣的同学可以深入了解，这里不再展示。走到这一步，流程图如下所示。 霍夫线段检测算法原理步骤如下： 初始化(θ,r)空间，N(θ,r)&#x3D;0 。（N(θ,r)表示在该参数表示的直线上的像素点的个数） 对于每一个像素点(x,y)，在参数空间中找出令 xcosθ+ysinθ&#x3D;r 的(θ,r)坐标，N(θ,r)+&#x3D;1 统计所有N(θ,r)的大小，取出N(θ,r)&gt;threasold的参数 。（threadsold是预设的阈值） OpenCV中封装好了基于霍夫变换的直线检测方法HoughLinesP，下面我们就来使用它进行线段检测。 123456789101112131415def detect_line(edges): &#x27;&#x27;&#x27; 基于霍夫变换的直线检测 &#x27;&#x27;&#x27; rho = 1 # 距离精度：1像素 angle = np.pi / 180 #角度精度：1度 min_thr = 10 #最少投票数 lines = cv2.HoughLinesP(edges, rho, angle, min_thr, np.array([]), minLineLength=8, maxLineGap=8) return lines 我们可以print一下lines，结果如下(此处仅作展示，后面有这一步的完整代码) 123456789101112131415161718192021[[[ 1 94 47 62]] [[143 94 156 103]] [[103 67 119 77]] [[ 1 86 41 60]] [[101 52 158 56]] [[104 69 159 100]] [[ 5 52 22 53]] [[129 63 140 63]] [[ 87 50 110 52]] [[ 0 88 17 77]] [[ 88 55 134 89]] [[ 2 94 36 70]] [[ 17 50 29 50]] [[ 23 73 42 60]] [[ 90 56 110 70]] [[ 1 56 16 51]] [[128 55 148 56]] [[ 0 89 8 84]] [[ 88 56 112 75]] [[151 101 159 104]] [[ 30 73 43 61]]] 返回的每组值都是一条线段表示线段起始位置(x_start,y_start,x_end,y_end)。可以看到小线段很多，我们对这些小线段做一下聚类和平均： 123456789101112131415161718192021222324252627def average_lines(frame, lines, direction=&#x27;left&#x27;): &#x27;&#x27;&#x27; 小线段聚类 &#x27;&#x27;&#x27; lane_lines = [] if lines is None: print(direction + &#x27;没有检测到线段&#x27;) return lane_lines height, width, _ = frame.shape fits = [] for line in lines: for x1, y1, x2, y2 in line: if x1 == x2: continue # 计算拟合直线 fit = np.polyfit((x1, x2), (y1, y2), 1) slope = fit[0] intercept = fit[1] if direction == &#x27;left&#x27; and slope &lt; 0: fits.append((slope, intercept)) elif direction == &#x27;right&#x27; and slope &gt; 0: fits.append((slope, intercept)) if len(fits) &gt; 0: fit_average = np.average(fits, axis=0) lane_lines.append(make_points(frame, fit_average)) return lane_lines 这里需要注意，由于图像的y坐标跟我们数学上经常遇到的y坐标方向是相反的（图像的y坐标轴正向是朝下的），因此，左侧黄色实线斜率是负值，右侧白色实线斜率是正值。上述代码我们将所有小线段的斜率和截距进行了平均，并且使用make_points函数重新计算了该平均线对应到图像上的起始坐标位置，make_points函数如下所示： 1234567891011def make_points(frame, line): &#x27;&#x27;&#x27; 根据直线斜率和截距计算线段起始坐标 &#x27;&#x27;&#x27; height, width, _ = frame.shape slope, intercept = line y1 = height y2 = int(y1 * 1 / 2) x1 = max(-width, min(2 * width, int((y1 - intercept) / slope))) x2 = max(-width, min(2 * width, int((y2 - intercept) / slope))) return [[x1, y1, x2, y2]] 上述函数最后返回的是坐标数值，这样看线段的坐标值不是很直观，我们可以写个脚本显式的观察这些线段： 1234567891011def display_line(frame, lines, line_color=(0, 0, 255), line_width=2): &#x27;&#x27;&#x27; 在原图上展示线段 &#x27;&#x27;&#x27; line_img = np.zeros_like(frame) if lines is not None: for line in lines: for x1, y1, x2, y2 in line: cv2.line(line_img, (x1, y1), (x2, y2), line_color, line_width) line_img = cv2.addWeighted(frame, 0.8, line_img, 1, 1) return line_img 上述代码我们将行道线按照一定权重与原图进行合成，方便我们查看最终效果。 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import cv2import numpy as npdef detect_line(edges): &#x27;&#x27;&#x27; 基于霍夫变换的直线检测 &#x27;&#x27;&#x27; rho = 1 # 距离精度：1像素 angle = np.pi / 180 # 角度精度：1度 min_thr = 10 # 最少投票数 lines = cv2.HoughLinesP(edges,rho, angle,min_thr,np.array([]),minLineLength=8,maxLineGap=8) return linesdef average_lines(frame, lines, direction=&#x27;left&#x27;): &#x27;&#x27;&#x27; 小线段聚类 &#x27;&#x27;&#x27; lane_lines = [] if lines is None: print(direction + &#x27;没有检测到线段&#x27;) return lane_lines #height, width , _= frame.shape fits = [] for line in lines: for x1, y1, x2, y2 in line: if x1 == x2: continue # 计算拟合直线 fit = np.polyfit((x1, x2), (y1, y2), 1) slope = fit[0] intercept = fit[1] if direction == &#x27;left&#x27; and slope &lt; 0: fits.append((slope, intercept)) elif direction == &#x27;right&#x27; and slope &gt; 0: fits.append((slope, intercept)) if len(fits) &gt; 0: fit_average = np.average(fits, axis=0) lane_lines.append(make_points(frame, fit_average)) return lane_linesdef make_points(frame, line): &#x27;&#x27;&#x27; 根据直线斜率和截距计算线段起始坐标 &#x27;&#x27;&#x27; height, width= frame.shape slope, intercept = line y1 = height y2 = int(y1 * 1 / 2) x1 = max(-width, min(2 * width, int((y1 - intercept) / slope))) x2 = max(-width, min(2 * width, int((y2 - intercept) / slope))) return [[x1, y1, x2, y2]]def display_line(frame, lines, line_color=(0,0,255), line_width=2): &#x27;&#x27;&#x27; 在原图上展示线段 &#x27;&#x27;&#x27; line_img = np.zeros_like(frame) if lines is not None: for line in lines: for x1, y1, x2, y2 in line: cv2.line(line_img, (x1, y1), (x2, y2), line_color, line_width) line_img = cv2.addWeighted(frame, 0.8, line_img, 1, 1) return line_imgdef main(): yimg = cv2.imread(&#x27;yellow_edge_new.jpg&#x27;,0) wimg = cv2.imread(&#x27;white_edge_new.jpg&#x27;,0) #霍夫变换 yimg_detect_line = detect_line(yimg) wimg_detect_line = detect_line(wimg) #小线段聚类 yimg_average_lines = average_lines(yimg,yimg_detect_line,direction=&#x27;right&#x27;) wimg_average_lines = average_lines(wimg,wimg_detect_line) #在原图上展示线段 y=cv2.imread(&#x27;test.jpg&#x27;) w=cv2.imread(&#x27;test.jpg&#x27;) yellow_display_line = display_line(y, yimg_average_lines) white_display_line = display_line(w, wimg_average_lines) cv2.imwrite(&#x27;yellow_display_line.jpg&#x27;, yellow_display_line) cv2.imwrite(&#x27;white_display_line.jpg&#x27;, white_display_line)if __name__==&quot;__main__&quot;: #函数入口 main() 我们来分析一下几段容易出错的代码： 123if len(fits) &gt; 0: fit_average = np.average(fits, axis=0) lane_lines.append(make_points(frame, fit_average)) 注意此处已经嵌套调用了make_points函数，返回了图片的x，y坐标。后续不需要再进行make_points，否则将会出现传入参数错误。 123def main(): yimg = cv2.imread(&#x27;yellow_edge_new.jpg&#x27;,0) wimg = cv2.imread(&#x27;white_edge_new.jpg&#x27;,0) 这里imread后面多带一个参数0，表示以单通道读入。即使图像显示为黑白，它仍然可能是三通道的，而yimg_detect_line中HoughLinesP霍夫变换只接受单通道的图片。这里不加0这个参数将会出现错误。 1234def display_line(frame, lines, line_color=(0,0,255), line_width=2): &#x27;&#x27;&#x27; 在原图上展示线段 &#x27;&#x27;&#x27; 注意line_color传入的rgb三色值是倒过来的，实际上红色对应的是(255，0，0)，而(0， 0， 255)是紫蓝色(?) 1height, width= frame.shape 某些.shape返回的是三个参数，第三个表示的是通道数。一般用_ 来接收这个不需要用到的通道数，而此处不会返回这个值，所以加入_ 会出错。实际应该根据编译器的提示进行修改(因为我也没摸透为什么，可能图片经过其他函数的一些转换后，不会返回这个参数。实际上修改起来也比较简单。) 1234567#在原图上展示线段 y=cv2.imread(&#x27;test.jpg&#x27;) w=cv2.imread(&#x27;test.jpg&#x27;) yellow_display_line = display_line(y, yimg_average_lines) white_display_line = display_line(w, wimg_average_lines) cv2.imwrite(&#x27;yellow_display_line.jpg&#x27;, yellow_display_line) cv2.imwrite(&#x27;white_display_line.jpg&#x27;, white_display_line) 这里读入我们拍摄的彩色图片，将红色的线段合成上去。整体代码运行结果如下： 从效果上看我们准确的将两条行道线检测了出来。接下来就是根据这两条行道线进行自动驾驶方向控制。 2.5动作控制：转向角针对前面的测试图片，我们可以有效的检测出两条行道线（左侧黄色线和右侧白色线），但是在真实的运行过程中，可能会出现3种情况： (1)正常检测到2条行道线：这种情况一般是直线车道且车辆稳定运行在行道线内，这时候我们只需要根据检测出的两条行道线微调整角度即可。 (2)检测出1条行道线：这种情况在转弯处容易出现，或者在车辆开始大范围偏离时出现，这时候我们的策略应该是向能够检测到的这条行道线方向前进。 (3)检测不到行道线：这种情况应该停下小车。 因此，针对三种情况我们需要不同的处理方式。代码如下所示： 12345678910111213141516171819202122232425# 计算转向角x_offset = 0y_offset = 0if len(yellow_lane)&gt;0 and len(white_lane)&gt;0: # 检测到2条线 _, _, left_x2, _ = yellow_lane[0][0] _, _, right_x2, _ = white_lane[0][0] mid = int(width / 2) x_offset = (left_x2 + right_x2) / 2 - mid y_offset = int(height / 2)elif len(yellow_lane)&gt;0 and len(yellow_lane[0])==1: # 只检测到黄色行道线 x1, _, x2, _ = yellow_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2)elif len(white_lane)&gt;0 and len(white_lane[0])==1: # 只检测到白色行道线 x1, _, x2, _ = white_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2)else: # 一条线都没检测到 print(&#x27;检测不到行道线，退出程序&#x27;) break angle_to_mid_radian = math.atan(x_offset / y_offset) angle_to_mid_deg = int(angle_to_mid_radian * 180.0 / math.pi) steering_angle = angle_to_mid_deg/45.0action = np.array([steering_angle, 0.3]) # 油门值恒定 到这里我们就可以开始启动程序了。完整代码如下所示： 先编写自定义库tools： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import cv2import numpy as npdef detect_line(edges): &#x27;&#x27;&#x27; 基于霍夫变换的直线检测 &#x27;&#x27;&#x27; rho = 1 # 距离精度：1像素 angle = np.pi / 180 # 角度精度：1度 min_thr = 10 # 最少投票数 lines = cv2.HoughLinesP(edges,rho, angle,min_thr,np.array([]),minLineLength=8,maxLineGap=8) return linesdef average_lines(frame, lines, direction=&#x27;left&#x27;): &#x27;&#x27;&#x27; 小线段聚类 &#x27;&#x27;&#x27; lane_lines = [] if lines is None: print(direction + &#x27;没有检测到线段&#x27;) return lane_lines #height, width , _= frame.shape fits = [] for line in lines: for x1, y1, x2, y2 in line: if x1 == x2: continue # 计算拟合直线 fit = np.polyfit((x1, x2), (y1, y2), 1) slope = fit[0] intercept = fit[1] if direction == &#x27;left&#x27; and slope &lt; 0: fits.append((slope, intercept)) elif direction == &#x27;right&#x27; and slope &gt; 0: fits.append((slope, intercept)) if len(fits) &gt; 0: fit_average = np.average(fits, axis=0) lane_lines.append(make_points(frame, fit_average)) return lane_linesdef make_points(frame, line): &#x27;&#x27;&#x27; 根据直线斜率和截距计算线段起始坐标 &#x27;&#x27;&#x27; height, width, _ = frame.shape slope, intercept = line y1 = height y2 = int(y1 * 1 / 2) x1 = max(-width, min(2 * width, int((y1 - intercept) / slope))) x2 = max(-width, min(2 * width, int((y2 - intercept) / slope))) return [[x1, y1, x2, y2]]def display_line(frame, lines, line_color=(0,0,255), line_width=2): &#x27;&#x27;&#x27; 在原图上展示线段 &#x27;&#x27;&#x27; line_img = np.zeros_like(frame) if lines is not None: for line in lines: for x1, y1, x2, y2 in line: cv2.line(line_img, (x1, y1), (x2, y2), line_color, line_width) line_img = cv2.addWeighted(frame, 0.8, line_img, 1, 1) return line_imgdef main(): yimg = cv2.imread(&#x27;yellow_edge_new.jpg&#x27;,0) wimg = cv2.imread(&#x27;white_edge_new.jpg&#x27;,0) #霍夫变换 yimg_detect_line = detect_line(yimg) wimg_detect_line = detect_line(wimg) #小线段聚类 yimg_average_lines = average_lines(yimg,yimg_detect_line,direction=&#x27;right&#x27;) wimg_average_lines = average_lines(wimg,wimg_detect_line) #在原图上展示线段 y=cv2.imread(&#x27;test.jpg&#x27;) w=cv2.imread(&#x27;test.jpg&#x27;) yellow_display_line = display_line(y, yimg_average_lines) white_display_line = display_line(w, wimg_average_lines) cv2.imwrite(&#x27;yellow_display_line.jpg&#x27;, yellow_display_line) cv2.imwrite(&#x27;white_display_line.jpg&#x27;, white_display_line)def region_of_interest(edges, color=&#x27;yellow&#x27;): &#x27;&#x27;&#x27; 感兴趣区域提取 &#x27;&#x27;&#x27; height, width = edges.shape mask = np.zeros_like(edges) # 定义感兴趣区域掩码轮廓 if color == &#x27;yellow&#x27;: polygon = np.array([[(width * 1 / 2, height * 1 / 2), (width, height * 1 / 2), (width, height), (width * 1 / 2, height)]], np.int32) else: polygon = np.array([[(0, height * 1 / 2), (width * 1 / 2, height * 1 / 2), (width * 1 / 2, height), (0, height)]], np.int32) # 填充感兴趣区域掩码 cv2.fillPoly(mask, polygon, (255, 255, 255)) # 提取感兴趣区域 croped_edge = cv2.bitwise_and(edges, mask) return croped_edge 再编写主函数入口： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# 导入系统库import cv2import numpy as npimport mathimport gymimport gym_donkeycar# 导入自定义库from tools import region_of_interest, detect_line, make_points, average_lines, display_linedef main(): &#x27;&#x27;&#x27; 主函数 &#x27;&#x27;&#x27; # 设置模拟器环境 env = gym.make(&quot;donkey-generated-roads-v0&quot;) # 重置当前场景 obv = env.reset() # 开始启动 action = np.array([0, 1]) # 动作控制，第1个转向值，第2个油门值 # 执行动作 obv, reward, done, info = env.step(action) # 获取图像 frame = cv2.cvtColor(obv, cv2.COLOR_RGB2BGR) # 运行1000次动作 for t in range(1000): # 转换图像到HSV空间 height, width, _ = frame.shape hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 黄色区域检测 lower_blue = np.array([15, 40, 40]) upper_blue = np.array([45, 255, 255]) yellow_mask = cv2.inRange(hsv, lower_blue, upper_blue) # 白色区域检测 lower_blue = np.array([0, 0, 200]) upper_blue = np.array([180, 30, 255]) white_mask = cv2.inRange(hsv, lower_blue, upper_blue) # 黄色线边缘提取 yellow_edge = cv2.Canny(yellow_mask, 200, 400) # 白色线边缘提取 white_edge = cv2.Canny(white_mask, 200, 400) # 黄色线感兴趣区域提取 yellow_roi = region_of_interest(yellow_edge, color=&#x27;yellow&#x27;) # 白色线感兴趣区域提取 white_roi = region_of_interest(white_edge, color=&#x27;white&#x27;) # 黄色线段检测 yellow_lines = detect_line(yellow_roi) yellow_lane = average_lines(frame, yellow_lines, direction=&#x27;right&#x27;) # yellow_show = display_line(frame, yellow_lane) # 白色线段检测 white_lines = detect_line(white_roi) white_lane = average_lines(frame, white_lines, direction=&#x27;left&#x27;) # white_show = display_line(frame, white_lane, line_color=(255, 0, 0)) # 计算转向角 x_offset = 0 y_offset = 0 if len(yellow_lane) &gt; 0 and len(white_lane) &gt; 0: # 检测到2条线 _, _, left_x2, _ = yellow_lane[0][0] _, _, right_x2, _ = white_lane[0][0] mid = int(width / 2) x_offset = (left_x2 + right_x2) / 2 - mid y_offset = int(height / 2) elif len(yellow_lane) &gt; 0 and len(yellow_lane[0]) == 1: # 只检测到黄色行道线 x1, _, x2, _ = yellow_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2) elif len(white_lane) &gt; 0 and len(white_lane[0]) == 1: # 只检测到白色行道线 x1, _, x2, _ = white_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2) else: # 一条线都没检测到 print(&#x27;检测不到行道线，退出程序&#x27;) break angle_to_mid_radian = math.atan(x_offset / y_offset) angle_to_mid_deg = int(angle_to_mid_radian * 180.0 / math.pi) steering_angle = angle_to_mid_deg / 45.0 action = np.array([steering_angle, 0.3]) # 油门值恒定 # 执行动作 obv, reward, done, info = env.step(action) # 重新获取图像 frame = cv2.cvtColor(obv, cv2.COLOR_RGB2BGR) # 运行完以后重置当前场景 obv = env.reset()if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 主函数入口 &#x27;&#x27;&#x27; main() 别忘了要先启动模拟器噢！ 到这里本节就已经接近尾声了。我对代码中的很多部分进行了修改，使它适应新版的opencv与实际情况。每一部分都重新编写了完整的测试代码。跳到这里没看的同学至少把测试代码运行一下，尝试理解原理。 注意：如果当前生成的赛道有“十字路口交叉”（每次重新进入赛道其生成的赛道都是随机绘制的），那么在运行的时候可能会出现失败、跑出赛道的现象。因为这种十字路口我们在程序中没有考虑。如何规避这个问题，有兴趣的同学可以自行研究。 本文更多的关注基于深度学习的图像处理技术，对于传统的图像处理算法（例如霍夫变换等）本文不再深入分析，同学们如果对这些传统图像处理算法不熟悉的可以自行再查阅资料深入研究，上面已经给出了不少资料的链接。 截止到目前为止，我们借助上面这个基于opencv的自动驾驶模拟平台，我们重新巩固了Python、opencv图像处理的基本使用方法，了解了自动驾驶项目的难点，对整个处理流程有了更进一步的认识。需要说明的是，尽管我们上述操作步骤是针对我们这个自动驾驶模拟平台的，但是以上步骤同样适用于很多其他图像处理任务，很多传统的图像处理任务都涵盖颜色空间变换、特定颜色物体提取、感兴趣区域过滤、霍夫变换等步骤，因此掌握上述常规的图像处理技术是非常重要的。 ##3.基于深度学习的自动驾驶控制 在上一节中我们通过OpenCV图像处理技术实现了一个简易的自动驾驶小车。但是很明显，这辆自动驾驶小车的适应性很差，当图像中有相同颜色的干扰物出现时，那么对于这辆自动驾驶的小车来说就是顶级灾难。另外，我们需要大量人工定义的参数，例如行道线颜色（黄色或白色）、颜色阈值、霍夫变换阈值等，而且一旦地图环境换了，所有这些参数我们都得重新手工调整，这些参数之间又有一定的耦合性，参数调整很麻烦。很显然，这种处理方法普适性不好。 那么能不能丢给机器一大堆图片，让机器自己去学习如何从当前图像中分析出小车应该转向的合适角度？如果没有接触过深度学习，那么乍一听这个想法简直是天方夜谭，然而深度学习确实做到了。这就是为什么近十年深度学习在图像处理领域取得了全面成功。深度学习能够从大量图像数据中自行学习高层次语义特征，完成媲美人类甚至超越人类的推理水平，整个学习过程不用人为干预，我们要做的就是“喂”一堆图片并且设定好需要优化的目标函数即可。当我们“喂”的图片越多、种类越丰富，那么最终机器学习到的驾驶水平越强，而且适应性越好。 本小节开始我们将正式进入基于深度学习的自动驾驶领域。 3.0一些基础知识根据后面要用到的网络，这里放上一些基础知识，篇幅都不(太)长，请同学们阅读。 神经网络与深度学习 这种基本的认识想必大家已经有了，但这里还是放一放。 pytorch基础教程 pytorch可以使用英伟达NVIDIA的产品进行GPU运算，但AMD锐龙的显卡并不支持。教程后期有教到如何使用。这是非常浅显易懂的课(用张老师的话来说就是傻瓜式的)，因此虽然我们主要用到的是pytorch但这里不进行教学。如果仍有困难，后面我会现场讲解或录视频，加上写文档的方式帮助同学们。 bp神经网络原理 bp神经网络我在导论课上讲的那个就是，忘了的话可以去导论那个群下载对应的excel演示下来玩一下。对应的论文也是非常有趣，预训练现在的应用非常广泛，同学们可以阅读一下。 卷积神经网络与池化、全连接、归一化 全连接神经网络 归一化 端到端是什么 同时建议大家复习一下我当时给大家讲python的时候说到的os库。 3.1算法原理本项目实现思路参考2016年英伟达发表的论文《End to End Learning for Self-Driving Cars》。这篇文章提出的方法核心思想就是使用神经网络自动提取图像特征，从传统的 image -&gt; features -&gt; action变成了image -&gt; action。该论文使用了深度网络结构，大大增强了图像特征提取能力，最终取得了不错的效果，其训练的模型不论是普通道路还是高速路，不论有道路标线还是没有道路标线都非常有效，解决了传统算法泛化性能差的问题。本文方法的测试性能非常好，在16年自动驾驶研究火热时，是一篇影响力很大的文章，即使放到现在，也是作为自动驾驶入门必读的Paper。 整个算法原理很简单，是对真实人类操作的一个模拟。对于我们人类驾驶员来说，假设我们正在驾驶这辆车，我们的执行流程跟上面算法也是一样的。首先我们用眼睛观看路面，然后我们的大脑根据当前眼睛看到的路面情况“下意识”的转动方向盘，转动一个我们认为合适的角度，从而避免车辆开出路面。这篇论文算法实现原理也是这样，具体如下图所示： 通过中间摄像头采集图像，然后图像输入到预先训练好的CNN网络，这个网络的输出是一个转向角度（可以理解为方向盘的转向角度），有了这个角度就可以控制小车按照这个角度进行转向。 有了这样一个模式，我们就只需要想办法训练这个CNN模型，针对每帧图像，都有一个我们认为合适的转向角度输出，即输入图像，输出一个回归值。具体模型结构如下图所示： 整个模型结构并不复杂，就是一堆的普通的CNN卷积神经网络模块按照顺序堆叠，最后使用全连接网络输出回归值。这个模型一共包含30层，由于其输入精度比较低(66x200)，因此推理速度也是比较快的，借助GPU可以实现实时推理。具体的，图像首先经过Normalization标准化，然后经过5组卷积层处理，最后拉平以后通过4个全连接层输出一个回归值，这个回归值就是我们项目中的转向角。 这里我们会遇到一个问题，训练上述深度神经网络我们需要大量的数据，即每帧图像以及对应的最佳转向值，这些数据怎么来呢？这篇论文里提出了一个方法，既然是模拟人类行为，那么只要让驾驶水平高超的“老司机”在相关赛道上进行手动驾驶，驾驶时一边记录每帧图像同时记录当前帧对应的操控的转向角，这样一组组数据记录下来就是我们认为的“最佳”训练数据。训练时，将模型预测的角度与给定图像帧的期望转向角度进行比较，误差通过反向传播反馈到CNN训练过程中，如下图所示。从图可以看出，这个过程在一个循环中重复，直到误差（本例中使用均方误差）足够低，这意味着模型已经学会了如何合理地转向。事实上，这是一个非常典型的图像分类训练过程，只不过这里预测输出是数值（回归值）而不是对象类别（分类概率）。 可以想象，如果能够完全的训练好这个模型，那么最终模型的输出结果是非常接近人类驾驶经验的。这篇论文通过大量实验证明，上述模型能够直接从拍摄的路面图像中有效的学习到最终的转向角，省去了传统算法颜色区域检测、感兴趣区域选择、霍夫变换等一系列复杂的耦合步骤。这篇论文做了一组实验，通过收集不到一百小时的少量训练数据进行训练，最后得到的模型足以支持在各种条件下操控车辆，比如高速公路、普通公路和居民区道路，以及晴天、多云和雨天等天气状况。 需要说明的是，这个模型的输出仅有一个转向角度，这样容易学习成功。如果输出变量再多一些（例如油门值、摄像头角度、行人避障等），那么这个模型还需要再进一步优化，感兴趣的同学可以借鉴近两年的论文进行深入研究（而我们的项目就是要做这个）。接下来我们就按照这个算法流程进行实现。 ###3.2数据采集 针对我们采用的自动驾驶模拟平台，为了能够采集到每帧图像及对应的最佳转向角度，我们可以使用前面第2节方法编写控制代码通过键盘控制小车（低匀速运行，仅仅只需要控制转向角度），然后记录每帧数据即可。这种模式是真实自动驾驶使用的，但是需要我们自己把自己练成经验充足的“老司机”，然后再去教会算法怎么驾驶。这样比较麻烦，这里可以有一种“偷懒”的办法。我们使用前面调参调的不错的OpenCV自动驾驶版本，使用OpenCV算法自动驾驶，然后记录每帧图像及对应角度。尽管这个OpenCV自动驾驶水平本身也一般（没有一直控制在两条行道线的绝对正中间），但是胜在能够基本稳定在行道线内。本文只是一个自动驾驶入门项目，可以采用这样的方法收集数据，来快速验证深度学习自动驾驶可行性。真实项目的话还是需要向“老司机”学习的。 先定义自定义tools库，这里对上面的进行了一定的修改。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import cv2import numpy as npdef region_of_interest(edges, color=&#x27;yellow&#x27;): &#x27;&#x27;&#x27; 感兴趣区域提取 &#x27;&#x27;&#x27; height, width = edges.shape mask = np.zeros_like(edges) # 定义感兴趣区域掩码轮廓 if color == &#x27;yellow&#x27;: polygon = np.array([[(0, height * 1 / 2), (width * 1 / 2, height * 1 / 2), (width * 1 / 2, height), (0, height)]], np.int32) else: polygon = np.array([[(width * 1 / 2, height * 1 / 2), (width, height * 1 / 2), (width, height), (width * 1 / 2, height)]], np.int32) # 填充感兴趣区域掩码 cv2.fillPoly(mask, polygon, 255) # 提取感兴趣区域 croped_edge = cv2.bitwise_and(edges, mask) return croped_edgedef detect_line(edges): &#x27;&#x27;&#x27; 基于霍夫变换的直线检测 &#x27;&#x27;&#x27; rho = 1 # 距离精度：1像素 angle = np.pi / 180 #角度精度：1度 min_thr = 10 #最少投票数 lines = cv2.HoughLinesP(edges, rho, angle, min_thr, np.array([]), minLineLength=8, maxLineGap=8) return linesdef average_lines(frame, lines, direction=&#x27;left&#x27;): &#x27;&#x27;&#x27; 小线段聚类 &#x27;&#x27;&#x27; lane_lines = [] if lines is None: print(direction + &#x27;没有检测到线段&#x27;) return lane_lines height, width, _ = frame.shape fits = [] for line in lines: for x1, y1, x2, y2 in line: if x1 == x2: continue # 计算拟合直线 fit = np.polyfit((x1, x2), (y1, y2), 1) slope = fit[0] intercept = fit[1] if direction == &#x27;left&#x27; and slope &lt; 0: fits.append((slope, intercept)) elif direction == &#x27;right&#x27; and slope &gt; 0: fits.append((slope, intercept)) if len(fits) &gt; 0: fit_average = np.average(fits, axis=0) lane_lines.append(make_points(frame, fit_average)) return lane_linesdef make_points(frame, line): &#x27;&#x27;&#x27; 根据直线斜率和截距计算线段起始坐标 &#x27;&#x27;&#x27; height, width, _ = frame.shape slope, intercept = line y1 = height y2 = int(y1 * 1 / 2) x1 = max(-width, min(2 * width, int((y1 - intercept) / slope))) x2 = max(-width, min(2 * width, int((y2 - intercept) / slope))) return [[x1, y1, x2, y2]]def display_line(frame, lines, line_color=(0, 0, 255), line_width=2): &#x27;&#x27;&#x27; 在原图上展示线段 &#x27;&#x27;&#x27; line_img = np.zeros_like(frame) if lines is not None: for line in lines: for x1, y1, x2, y2 in line: cv2.line(line_img, (x1, y1), (x2, y2), line_color, line_width) line_img = cv2.addWeighted(frame, 0.8, line_img, 1, 1) return line_img 完整采集代码如下，同样有一定的变化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# 导入系统库import cv2import numpy as npimport mathimport gymimport gym_donkeycar # 导入自定义库from tools import region_of_interest, detect_line, make_points, average_lines, display_line def main(): &#x27;&#x27;&#x27; 主函数 &#x27;&#x27;&#x27; # 设置模拟器环境 env = gym.make(&quot;donkey-generated-roads-v0&quot;) # 重置当前场景 obv = env.reset() # 开始启动 action = np.array([0, 0.3]) # 动作控制，第1个转向值，第2个油门值 # 执行动作 obv, reward, done, info = env.step(action) # 获取图像 frame = cv2.cvtColor(obv, cv2.COLOR_RGB2BGR) # 运行4000次动作 pic_index = 0 for t in range(4000): # 转换图像到HSV空间 height, width, _ = frame.shape hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 黄色区域检测 lower_blue = np.array([15, 40, 40]) upper_blue = np.array([45, 255, 255]) yellow_mask = cv2.inRange(hsv, lower_blue, upper_blue) # 白色区域检测 lower_blue = np.array([0, 0, 200]) upper_blue = np.array([180, 30, 255]) white_mask = cv2.inRange(hsv, lower_blue, upper_blue) # 黄色线边缘提取 yellow_edge = cv2.Canny(yellow_mask, 200, 400) # 白色线边缘提取 white_edge = cv2.Canny(white_mask, 200, 400) # 黄色线感兴趣区域提取 yellow_roi = region_of_interest(yellow_edge, color=&#x27;yellow&#x27;) # 白色线感兴趣区域提取 white_roi = region_of_interest(white_edge, color=&#x27;white&#x27;) # 黄色线段检测 yellow_lines = detect_line(yellow_roi) yellow_lane = average_lines(frame, yellow_lines, direction=&#x27;left&#x27;) #yellow_show = display_line(frame, yellow_lane) # 白色线段检测 white_lines = detect_line(white_roi) white_lane = average_lines(frame, white_lines, direction=&#x27;right&#x27;) #white_show = display_line(frame, white_lane, line_color=(255, 0, 0)) # 计算转向角 x_offset = 0 y_offset = 0 if len(yellow_lane) &gt; 0 and len(white_lane) &gt; 0: # 检测到2条线 _, _, left_x2, _ = yellow_lane[0][0] _, _, right_x2, _ = white_lane[0][0] mid = int(width / 2) x_offset = (left_x2 + right_x2) / 2 - mid y_offset = int(height / 2) elif len(yellow_lane) &gt; 0 and len(yellow_lane[0]) == 1: # 只检测到黄色行道线 x1, _, x2, _ = yellow_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2) elif len(white_lane) &gt; 0 and len(white_lane[0]) == 1: # 只检测到白色行道线 x1, _, x2, _ = white_lane[0][0] x_offset = x2 - x1 y_offset = int(height / 2) else: # 一条线都没检测到 print(&#x27;检测不到行道线，退出程序&#x27;) break angle_to_mid_radian = math.atan(x_offset / y_offset) angle_to_mid_deg = int(angle_to_mid_radian * 180.0 / math.pi) steering_angle = angle_to_mid_deg / 45.0 action = np.array([steering_angle, 0.1]) # 油门值恒定 # 记录当前图像和转向角度 img_path = &quot;log/&#123;:d&#125;_&#123;:.4f&#125;.jpg&quot;.format(pic_index, steering_angle) cv2.imwrite(img_path, frame) pic_index += 1 # 执行动作 obv, reward, done, info = env.step(action) # 重新获取图像 frame = cv2.cvtColor(obv, cv2.COLOR_RGB2BGR) # 运行完以后重置当前场景 print(&#x27;结束本次采集&#x27;) obv = env.reset() if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 主函数入口 &#x27;&#x27;&#x27; main() 图片名采用“图片帧号_转向角度.jpg”的形式命名。上述代码每次跑完会在log目录下生成4000多张图片。由于每次的地图都是随机生成的，因此我们可以多跑几次，多收集一些数据。 最终共采集10个文件夹图片，总共4万张图片： 接下来我们需要对这些图片进行整理，拆分数据集用于训练和验证。我们把这些文件夹移动到data&#x2F;simulate下，注意这个文件夹需要自己手动创建。 详细脚本代码create_data_lists.py如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 导入系统库import osimport random def creat_data_list(dataset_path, file_list, mode=&#x27;train&#x27;): &#x27;&#x27;&#x27; 创建txt文件列表 &#x27;&#x27;&#x27; with open(os.path.join(dataset_path, (mode + &#x27;.txt&#x27;)), &#x27;w&#x27;) as f: for (imgpath, angle) in file_list: f.write(imgpath + &#x27; &#x27; + str(angle) + &#x27;\\n&#x27;) print(mode + &#x27;.txt 已生成&#x27;) def getFileList(dir, Filelist, ext=None): &quot;&quot;&quot; 获取文件夹及其子文件夹中文件列表 输入 dir: 文件夹根目录 输入 ext: 扩展名 返回: 文件路径列表 &quot;&quot;&quot; newDir = dir if os.path.isfile(dir): if ext is None: Filelist.append(dir) else: if ext in dir[-3:]: Filelist.append(dir) elif os.path.isdir(dir): for s in os.listdir(dir): newDir = os.path.join(dir, s) getFileList(newDir, Filelist, ext) return Filelist def main(): &#x27;&#x27;&#x27; 主函数 &#x27;&#x27;&#x27; # 设置参数 org_img_folder = &#x27;./data/simulate&#x27; # 数据集根目录 train_ratio = 0.8 # 训练集占比 # 检索jpg文件 jpglist = getFileList(org_img_folder, [], &#x27;jpg&#x27;) print(&#x27;本次执行检索到 &#x27; + str(len(jpglist)) + &#x27; 个jpg文件\\n&#x27;) file_list = list() # 解析转向值 for jpgpath in jpglist: print(jpgpath) curDataDir = os.path.dirname(jpgpath) basename = os.path.basename(jpgpath) angle = (basename[:-4]).split(&#x27;_&#x27;)[-1] imgPath = os.path.join(curDataDir, basename).replace(&quot;\\\\&quot;, &quot;/&quot;) file_list.append((imgPath, angle)) # 切分数据 random.seed(256) random.shuffle(file_list) train_num = int(len(file_list) * train_ratio) train_list = file_list[0:train_num] val_list = file_list[train_num:] # 创建列表文件 creat_data_list(org_img_folder, train_list, mode=&#x27;train&#x27;) creat_data_list(org_img_folder, val_list, mode=&#x27;val&#x27;) if __name__ == &quot;__main__&quot;: &#x27;&#x27;&#x27; 程序入口 &#x27;&#x27;&#x27; main() 上述代码我们查找每个log文件夹下的jpg文件，然后解析出对应的转向值。将这些值最后分别保存到train.txt和val.txt文件中。在代码里面，我们设定训练集占比0.8，剩下的0.2则为验证集。 生成的train.txt和val.txt文件每行内容表示一个样本，由图片路径和转向值组成，中间用空格隔开。 接下来我们将使用Pytoch框架实现深度学习算法进行训练、验证。 3.3模型训练 首先定义数据采集器datasets.py，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 导入系统库import osimport numpy as npimport cv2 # 导入PyTorch库import torchfrom torch.utils.data import Dataset class AutoDriveDataset(Dataset): &quot;&quot;&quot; 数据集加载器 &quot;&quot;&quot; def __init__(self, data_folder, mode, transform=None): &quot;&quot;&quot; :参数 data_folder: # 数据文件所在文件夹根路径(train.txt和val.txt所在文件夹路径) :参数 mode: &#x27;train&#x27; 或者 &#x27;val&#x27; :参数 normalize_type: 图像归一化处理方式 &quot;&quot;&quot; self.data_folder = data_folder self.mode = mode.lower() self.transform = transform assert self.mode in &#123;&#x27;train&#x27;, &#x27;val&#x27;&#125; # 读取图像列表路径 if self.mode == &#x27;train&#x27;: file_path=os.path.join(data_folder, &#x27;train.txt&#x27;) else: file_path=os.path.join(data_folder, &#x27;val.txt&#x27;) self.file_list=list() with open(file_path, &#x27;r&#x27;) as f: files = f.readlines() for file in files: if file.strip() is None: continue self.file_list.append([file.split(&#x27; &#x27;)[0],float(file.split(&#x27; &#x27;)[1])]) def __getitem__(self, i): &quot;&quot;&quot; :参数 i: 图像检索号 :返回: 返回第i个图像和标签 &quot;&quot;&quot; # 读取图像 img = cv2.imread(self.file_list[i][0]) img = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) if self.transform: img = self.transform(img) # 读取标签 label = self.file_list[i][1] label = torch.from_numpy(np.array([label])).float() return img, label def __len__(self): &quot;&quot;&quot; 为了使用PyTorch的DataLoader,必须提供该方法. :返回: 加载的图像总数 &quot;&quot;&quot; return len(self.file_list) 上述代码比较简单，我们构造了AutoDriveDataset类用于作为自动驾驶小车数据读取类，从train.txt和val.txt中根据每行内容得到每个样本的图像路径和对应的真值标签。这里需要注意下颜色空间，我们最终是使用HSV空间进行训练的，因此需要做一下转化。 这里插播一下一个自定义的方法库utils.py： 123456789101112131415161718class AverageMeter(object): &#x27;&#x27;&#x27; 平均器类,用于计算平均值、总和 &#x27;&#x27;&#x27; def __init__(self): self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count 虽然这个文件里面只有一个类，但创建这样一个文件是习惯。 有了数据读取类以后我们就下来定义模型，具体代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041# 导入PyTorch库import torch.nn as nnimport torch.nn.functional as F class AutoDriveNet(nn.Module): &#x27;&#x27;&#x27; 端到端自动驾驶模型 &#x27;&#x27;&#x27; def __init__(self): &quot;&quot;&quot; 初始化 &quot;&quot;&quot; super(AutoDriveNet, self).__init__() self.conv_layers = nn.Sequential(nn.Conv2d(3, 24, 5, stride=2), nn.ELU(), nn.Conv2d(24, 36, 5, stride=2), nn.ELU(), nn.Conv2d(36, 48, 5, stride=2), nn.ELU(), nn.Conv2d(48, 64, 3), nn.ELU(), nn.Conv2d(64, 64, 3), nn.Dropout(0.5)) self.linear_layers = nn.Sequential( #nn.Linear(in_features=64 * 2 * 33, out_features=100), nn.Linear(in_features=64 * 8 * 13, out_features=100), nn.ELU(), nn.Linear(in_features=100, out_features=50), nn.ELU(), nn.Linear(in_features=50, out_features=10), nn.Linear(in_features=10, out_features=1)) def forward(self, input): &#x27;&#x27;&#x27; 前向推理 &#x27;&#x27;&#x27; input = input.view(input.size(0), 3, 120, 160) output = self.conv_layers(input) output = output.view(output.size(0), -1) output = self.linear_layers(output) return output 这里需要注意的是我们的模型跟论文里的稍微有点不一样(见下)，主要是因为我们的图像尺寸是120x160的，而论文里使用的是66x200。因此，我们对应的输入需要调整下，另外，在最后全连接层也相应的在维度上要调整。对于实际项目来说，现在很多的摄像头都是使用3:4分辨率的，例如树莓派摄像头典型的分辨率是480x640，因此，修改过后的模型更具有普遍性，还方便后面迁移到真实环境训练。 12345678910111213141516self.conv_layers = nn.Sequential(nn.Conv2d(3, 24, 5, stride=2), nn.ELU(), nn.Conv2d(24, 36, 5, stride=2), nn.ELU(), nn.Conv2d(36, 48, 5, stride=2), nn.ELU(), nn.Conv2d(48, 64, 3), nn.ELU(), nn.Conv2d(64, 64, 3), nn.Dropout(0.5))self.linear_layers = nn.Sequential( #nn.Linear(in_features=64 * 2 * 33, out_features=100), nn.Linear(in_features=64 * 8 * 13, out_features=100), nn.ELU(), nn.Linear(in_features=100, out_features=50), nn.ELU(), nn.Linear(in_features=50, out_features=10), nn.Linear(in_features=10, out_features=1)) 整个模型比较简单，前面是多个cnn，最后接几个全连接网络，输入是3通道图像，输出是一个转向回归值。 训练脚本代码train.py如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135# 导入torch库import torch.backends.cudnn as cudnnimport torchfrom torch import nnimport torchvision.transforms as transformsfrom torch.utils.tensorboard import SummaryWriter# 导入自定义库from models import AutoDriveNetfrom datasets import AutoDriveDatasetfrom utils import *def main(): &quot;&quot;&quot; 训练. &quot;&quot;&quot; # 数据集路径 data_folder = &#x27;./data/simulate&#x27; # 学习参数 checkpoint = None # 预训练模型路径，如果不存在则为None # checkpoint = &quot;./results/checkpoint.pth&quot; batch_size = 400 # 批大小 start_epoch = 1 # 轮数起始位置 epochs = 1000 # 迭代轮数 lr = 1e-4 # 学习率 # 设备参数 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(torch.cuda.is_available(), device) #ngpu = 4 # 用来运行的gpu数量 cudnn.benchmark = True # 对卷积进行加速 writer = SummaryWriter() # 实时监控 使用命令 tensorboard --logdir runs 进行查看 # 初始化模型 model = AutoDriveNet() # 初始化优化器 optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr) # 迁移至默认设备进行训练 model = model.to(device) model = model.cuda() criterion = nn.MSELoss().to(device) criterion = nn.MSELoss().cuda() # 加载预训练模型 if checkpoint is not None: checkpoint = torch.load(checkpoint) start_epoch = checkpoint[&#x27;epoch&#x27;] + 1 model.load_state_dict(checkpoint[&#x27;model&#x27;]) optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;]) # 单机多卡训练 # if torch.cuda.is_available(): # model = nn.DataParallel(model, device_ids=list(range(ngpu))) # model = nn.DataParallel(model, device_ids=[0]) # 定制化的dataloader transformations = transforms.Compose([ transforms.ToTensor(), # 通道置前并且将0-255RGB值映射至0-1 # transforms.Normalize( # mean=[0.485, 0.456, 0.406], # 归一化至[-1,1] mean std 来自imagenet 计算 # std=[0.229, 0.224, 0.225]) ]) train_dataset = AutoDriveDataset(data_folder, mode=&#x27;train&#x27;, transform=transformations) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) # 开始逐轮训练 for epoch in range(start_epoch, epochs + 1): model.train() # 训练模式：允许使用批样本归一化 loss_epoch = AverageMeter() # 统计损失函数 n_iter = len(train_loader) # 按批处理 for i, (imgs, labels) in enumerate(train_loader): # 数据移至默认设备进行训练 imgs = imgs.to(device) labels = labels.to(device) # 前向传播 pre_labels = model(imgs) # 计算损失 loss = criterion(pre_labels, labels) # 后向传播 optimizer.zero_grad() loss.backward() # 更新模型 optimizer.step() # 记录损失值 loss_epoch.update(loss.item(), imgs.size(0)) # 打印结果 print(&quot;第 &quot; + str(i) + &quot; 个batch训练结束&quot;) # 手动释放内存 del imgs, labels, pre_labels # 监控损失值变化 writer.add_scalar(&#x27;MSE_Loss&#x27;, loss_epoch.avg, epoch) print(&#x27;epoch:&#x27; + str(epoch) + &#x27; MSE_Loss:&#x27; + str(loss_epoch.avg)) # 保存预训练模型 torch.save( &#123; &#x27;epoch&#x27;: epoch, # &#x27;model&#x27;: model.module.state_dict(), &#x27;model&#x27;: model.state_dict(), &#x27;optimizer&#x27;: optimizer.state_dict() &#125;, &#x27;results/checkpoint.pth&#x27;) # 训练结束关闭监控 writer.close()if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 程序入口 &#x27;&#x27;&#x27; main() 新旧版更替，不同的运行环境都有可能导致代码报各种错误。这里真是坑到不行qwq。经过我反复测试，大家报错时可以解开就近的注释部分进行尝试，这应该是目前最为稳定的版本了。 注意： 12writer = SummaryWriter()writer.add_scalar(&#x27;MSE_Loss&#x27;, loss_epoch.avg, epoch) 第一行代码及下面的相关方法使得我们可以在pycharm下方终端使用命令 tensorboard –logdir runs ，点击出现的网页链接(用edge或者chome)，可以看到误差改变情况。如果这行代码报错了，请在终端运行命令 pip install tensorboard等待安装完成。反复刷新浏览器可以更新。根据网页提示进行使用。如果测试的时候运行过太多次，可以修改第二行代码的字符串，会生成一个新的表。删除根目录下run的文件也可以起到减少的作用。 12345#ngpu = 4 # 用来运行的gpu数量# 单机多卡训练 # if torch.cuda.is_available(): # model = nn.DataParallel(model, device_ids=list(range(ngpu))) # model = nn.DataParallel(model, device_ids=[0]) 只有一个显卡的同学一定要注意保持所有单机多卡训练的代码处于注释状态(我在这儿被坑惨了) 1234567torch.save( &#123; &#x27;epoch&#x27;: epoch, # &#x27;model&#x27;: model.module.state_dict(), &#x27;model&#x27;: model.state_dict(), &#x27;optimizer&#x27;: optimizer.state_dict() &#125;, &#x27;results/checkpoint.pth&#x27;) 对model字段的保存，有的版本需要用注释里的字段，有的用当前的就行。请注意results文件夹最好手动创建。 3.4模型验证上面的代码需要迭代1000次, epoch&#x3D;1000的时候基本处在一个比较好的收敛位置，此时误差下降到1e-5左右。不过在跑到第140次左右时就已经下降到1e-4，让误差下降一个数量级在实际工程中还是非常重要的，不过同学们在使用的时候其实跑到这样也能用了。 验证代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# 导入系统库import time# 导入PyTorch库import torchfrom torch import nnimport torch.backends.cudnn as cudnnimport torchvision.transforms as transforms# 导入自定义库from datasets import AutoDriveDatasetfrom models import AutoDriveNetfrom utils import *def main(): # 测试集目录 data_folder = &quot;./data/simulate&quot; # 定义运行的GPU数量 ngpu = 1 # cudnn.benchmark = True # 定义设备运行环境 print(&quot;定义设备环境&quot;) device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 加载预训练模型 print(&quot;加载预训练模型&quot;) checkpoint = torch.load(&quot;./results/checkpoint.pth&quot;) model = AutoDriveNet() model = model.to(device) model.load_state_dict(checkpoint[&#x27;model&#x27;]) # 多GPU封装 #if torch.cuda.is_available(): # model = nn.DataParallel(model, device_ids=list(range(ngpu))) # 定制化的dataloader # 定制dataloader transformations = transforms.Compose([ transforms.ToTensor(), # 通道置前并且将0-255RGB值映射至0-1 # transforms.Normalize( # mean=[0.485, 0.456, 0.406], # 归一化至[-1,1] mean std 来自imagenet 计算 # std=[0.229, 0.224, 0.225]) ]) val_dataset = AutoDriveDataset(data_folder, mode=&#x27;val&#x27;, transform=transformations ) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1, pin_memory=True) # 定义评估指标 criterion = nn.MSELoss().to(device) # 记录误差值 MSEs = AverageMeter() # 记录测试时间 model.eval() start = time.time() with torch.no_grad(): print(&quot;开始进行测试&quot;) # 逐批样本进行推理计算 for i, (imgs, labels) in enumerate(val_loader): print(&quot;第&#123;&#125;次计算进行中&quot;.format(i)) # 数据移至默认设备进行推理 imgs = imgs.to(device) labels = labels.to(device) # 前向传播 pre_labels = model(imgs) # 计算误差 loss = criterion(pre_labels, labels) MSEs.update(loss.item(), imgs.size(0)) # 输出平均均方误差 print(&#x27;MSE &#123;mses.avg: .3f&#125;&#x27;.format(mses=MSEs)) print(&#x27;平均单张样本用时 &#123;:.3f&#125; 秒&#x27;.format((time.time() - start) / len(val_dataset)))if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 程序入口 &#x27;&#x27;&#x27; main() 运行结果如下： 我们的转向角度取值范围是[-1, 1]，这样的误差比较小，是可以接受的。请注意多卡训练的同学，一定要解开nn.DataParallel的注释，否则是无法运行的。 这里等待的时间比较长，所以我增加了一些print输出来判断到底是电脑卡了还是程序在运行。同学们如果配置比较低，也建议这样自行增加一些print。 3.5单张图片预测为什么上一步已经用8000张图片进行预测了，这里还要用单张图片来尝试呢？真实的自动驾驶的时候，我们是要对每一张图片进行分析的。所以这里来尝试一下用单张图片来分析。 我们选取一张比较有代表性的图片: 大家可以先猜测一下这个转向角度。 下面是测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 导入OpenCV库import cv2 # 导入PyTorch库from torch import nnimport torch # 导入自定义库from models import AutoDriveNetfrom utils import * def main(): &#x27;&#x27;&#x27; 主函数 &#x27;&#x27;&#x27; # 测试图像 imgPath = &#x27;./results/test.jpg&#x27; # 推理环境 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 加载训练好的模型 checkpoint = torch.load(&#x27;./results/checkpoint.pth&#x27;) model = AutoDriveNet() model = model.to(device) model.load_state_dict(checkpoint[&#x27;model&#x27;],strict=False) # 加载图像 img = cv2.imread(imgPath) img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # 图像预处理 # PIXEL_MEANS = (0.485, 0.456, 0.406) # RGB格式的均值和方差 # PIXEL_STDS = (0.229, 0.224, 0.225) img = torch.from_numpy(img.copy()).float() img /= 255.0 # img -= torch.tensor(PIXEL_MEANS) # img /= torch.tensor(PIXEL_STDS) img = img.permute(2, 0, 1) img.unsqueeze_(0) # 转移数据至设备 img = img.to(device) # 模型推理 model.eval() with torch.no_grad(): prelabel = model(img).squeeze(0).cpu().detach().numpy() print(&#x27;预测结果 &#123;:.3f&#125; &#x27;.format(prelabel[0])) if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 程序入口 &#x27;&#x27;&#x27; main() 请注意一定要把测试的图片移动到result目录下，且命名为test.jpg，或修改代码中对应行。 测试结果： 而实际上这台小车正准备右转，转向值为0.244。趋势上是正确的，而效果上也是可以接受的。 3.6系统集成，自动驾驶我们现在可以使用pytorch逐帧分析图像，然后直接给出转向值用于小车控制，不再需要复杂的、分散的图像处理步骤。 只需要把上一小节的代码和之前的控制代码合并即可实现。编程能力强的同学可以留作练习。 运行auto_drive.py文件，其完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# 导入系统库import cv2import numpy as npimport mathimport gymimport gym_donkeycar # 导入PyTorch库from torch import nnimport torch # 导入自定义库from models import AutoDriveNetfrom utils import * def main(): &#x27;&#x27;&#x27; 主函数 &#x27;&#x27;&#x27; # 设置模拟器环境 env = gym.make(&quot;donkey-generated-roads-v0&quot;) # 设置推理环境 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 加载训练好的模型 checkpoint = torch.load(&#x27;./results/checkpoint.pth&#x27;) model = AutoDriveNet() model = model.to(device) model.load_state_dict(checkpoint[&#x27;model&#x27;]) # 重置当前场景 obv = env.reset() # 开始启动 action = np.array([0, 0.1]) # 动作控制，第1个转向值，第2个油门值 # 执行动作并获取图像 img, reward, done, info = env.step(action) img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) # 运行5000次动作 model.eval() for t in range(5000): # 图像预处理 img = torch.from_numpy(img.copy()).float() img /= 255.0 img = img.permute(2, 0, 1) img.unsqueeze_(0) # 转移数据至设备 img = img.to(device) # 模型推理 steering_angle = 0 factor=1 with torch.no_grad(): # 计算转向角度 steering_angle = (model(img).squeeze(0).cpu().detach().numpy())[0] if steering_angle*factor&lt;-1: steering_angle=-1 elif steering_angle*factor&gt;1: steering_angle=1 else: steering_angle=steering_angle*factor print(steering_angle) action = np.array([steering_angle, 0.1]) # 油门值恒定 # 执行动作并更新图像 img, reward, done, info = env.step(action) img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) # 运行完以后重置当前场景 obv = env.reset() if __name__ == &#x27;__main__&#x27;: &#x27;&#x27;&#x27; 主函数入口 &#x27;&#x27;&#x27; main() 从视频效果上看，通过深度学习的自动驾驶小车其操控流畅性感觉上超过了它的“师傅”OpenCV版本。可能的原因在于纯粹的OpenCV图像处理方法对每帧单独处理，没有一个整体的去噪概念，容易在某一帧出现偏差。但是基于深度学习的方法更多的是学习整个数据集的操作体验，某种意义上做了一定的概率去噪，或者说是平均化，因此，整个的操控才会显得更加流畅。 到这里，这个简单的教程就结束了。恭喜你，你已经入门了end to end自动驾驶(大概)。希望我的拙见可以帮助到大家学习。","categories":[],"tags":[]},{"title":"MySQL","slug":"MySQL","date":"2023-03-27T08:20:58.000Z","updated":"2023-03-27T08:24:16.643Z","comments":true,"path":"2023/03/27/MySQL/","link":"","permalink":"http://example.com/2023/03/27/MySQL/","excerpt":"","text":"安装 到官网进行安装。点击MySQL Installer for Windows下载安装包后一直next即可。 启动与停止 mysql开机默认启动。 在win+R运行services.msc，找到mysql右键操作。 在cmd中运行net start mysql80、net stop mysql80进行启动与停止。 客户端连接 方式一：直接使用mysql提供的命令行进行连接。开始找到MySQL Command Line Client-Unicode，进去输入密码操作。 方式二：先配置PATH环境变量C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin。在任意cmd输入mysql -u root -p，输入密码操作。 MySQL数据库的好处 关系型数据库RDBMS，建立在关系模型基础上，由多张相互连接的二维表组成的数据库。格式统一，便于维护，使用SQL语言操作，使用方便。数据模型是数据库，可以创建多个表。 SQL （1） SQL通用语法 （2） SQL分类 （3） DDL ​ 查询数据库 12show databases;select database();# 查询当前数据库 ​ 如果创建了同样名称的数据库会报错。一般使用以下指令创建。 1create database if not exists; ​ 在创建数据库的时候可以指定字符集。utf8只占有三个字节，而某些数据会占据四个字节，因此一般设置为utf8mb4 1create database if not exists default charset utf8mb4; ​ 删除数据库 删除不存在的数据库同样会报错，因此使用以下指令进行删除 1DROP database IF EXISTS databaseName; ​ 数据库使用 1USE databaseName; ​ 表创建 数据类型quality ​ 1）数值类型 ​ 定义float和double类型时，需要两个参数。如score double(4, 1)代表4个长度，最多1位小数。 ​ 定义无符号时应该是 age TINYINT UNSIGNED;,unsigned放在数据类型后面。 ​ 2） 字符串类型 ​ sql中字符串为varchar(size)，size是字符串的长度。一旦超出size会报错。varchar会动态占用内存，而char不管多大都必定占用size内存。但是char性能更高。 ​ 例如用户名适合用varchar，性别适合用char。 ​ 3） 日期类型 birthday date; 注意最后一个字段没有逗号。 12345CREATE TABLE tableName( 字段1 字段1类型 COMMENT “字段1注释”, ... 字段n 字段n类型 COMMENT “字段n注释”)COMMENT “表注释”； ​ 查询当前数据库所有表，需要先用use指令进入数据库。 1SHOW TABLES; ​ 查询表结构 1DESC databaseName; ​ 查询指定表的建表语句，用于展示详细注释 1SHOW CREATE TABLE databaseName; 案例 ​ 表修改 1）添加字段 1ALTER TABLE tableName ADD fieldName quality COMMENT “注释” 约束; ​ 2） 修改数据类型 1ALTER TABLE tableName MDOIFY fieldName newQuality(size); ​ 3） 修改字段名和字段类型 1ALTER TABLE tableName CHANGE oldTableName newTableName quality(size) COMMENT &quot;注释&quot; 约束; ​ 4) 删除字段 1ALTER TABLE tableName DROP filedName; ​ 5) 修改表名 1ALTER TABLE tableName RENAME TO newTableName; ​ 6) 删除表 1DROP TABLE IF EXISTS tableName; ​ 7) 删除指定表，并重新创建空的表结构 1TRUNCATE TABLE tableName; 图形化工具DataGrip 进入后点击左上角加号，新增mysql Data Sources ,配置文件。user填root，密码随便设置，确认后下载驱动文件。 ​ 1） 创建数据库 右键 new 选择schema，填写名字。 2）新建表 右键数据库 创建表，填写名字和注释信息。在colums中创建字段。点击execute。 ​ 3）修改表 右键数据库 modify table ​ 4）使用sql语句操作 右键 new quary console ​ （4）DML 增删改操作 如果没有where条件，会修改所有数据。 （5）DQL 查操作 关键字：SELECT 1）基础查询 查询可以起别名。SELECT … as ‘ ,,, ‘ from …;其中as可以省略。 查询如果不要重复，可以在select后加distinct关键字。 聚合函数 count不会统计null，最好采用count(*) 2） 分组查询 3） 排序查询 如果是多字段排序，字段一 一样时才会用第二种排序 4） 分页查询 编写顺序与执行顺序 （6）DCL 管理数据库用户、控制数据库访问权限 1） 用户 2） 权限 主机名可以用’%’通配，表名用’*’通配。 函数 mysql内置了函数。select 函数名(); （1）字符串函数 （2）数值函数 （3）日期函数 type填写YEAR、MONTH、DAY （4）流程函数 约束 保证数据的正确性和完整性 尽管没有插入成功，数据也会向mysql申请主键。 在可视化创建数据的时候可以直接勾选。 外键关联 表之间的联系 事务 要么全部成功，要么全部失败 方式一 ​ 方式二 事务的四大特性 并发事务问题 解决方法：事务的隔离级别 python 3中运行pip install Mysqlclient, 通过import MySQLdb来建立数据库的连接。","categories":[],"tags":[]},{"title":"面向对象","slug":"面向对象","date":"2023-03-22T12:42:09.000Z","updated":"2023-03-22T12:45:12.496Z","comments":true,"path":"2023/03/22/面向对象/","link":"","permalink":"http://example.com/2023/03/22/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"explicit关键字，作为返回值时加上不调用拷贝构造函数。作为类关键字防止被拷贝。 多态父类指针指向子类对象 父类提供virtual 虚函数，子类对象实现 析构函数加virtual，可以调用子类的析构函数 构造函数前不能加virtual，因为构造函数执行后，虚表才出现 运算符重载 类中，++i比i++效率高，因为后者还要调用构造函数 工厂模式用父类指针作为返回值，在返回值创建子类对象 单体模式sizeof、_countof_tscanf_s( _T(“%d”), v1, _countof(v1)); sizeof 数据类型大小与数据大小相乘，实际的数据大小 countof 数据大小，实际上的数据数量 结构体对齐大小为4，1，8，按最大的对齐 如上，实际上为16。double占8，另外两个加起来用8 重载函数名一样，参数类型不同，参数数量不同 与返回值类型无关 使用const，发生重载。 const是常对象调用的。 菱形继承一个派生类D是由多继承产生的，它的多个基类B、C继承了同一个基类A。造成派生类D中具有多份A类的属性，属于一种不合理的现象 会让高层的基类在底层的派生类中拥有多份成员，造成二义性。 解决方法：使用作用域、利用虚继承 在使用时加上作用域 但是不能解决多次拷贝的问题 虚继承 给所有函数都加上virtual 继承时使用虚继承 使用虚继承时，第一成员是一个指针，指向一个结构体，里面第二成员是一个十六进制偏移。偏移值是父类到子类的偏移。 虚继承中，父类指针只能访问子类对象被继承的部分。因为在定义的时候，有一个偏移指向对应的数据。 虚表将父类中函数定义为虚函数后，会出现一个虚表指针（4或8）在上述结构体的第一成员（第二成员是到子类的偏移），指向一个数组。数组中存放的都是函数，如果孩子实现了这个函数，就会放入孩子的。如果孩子的没实现，就会放入自己的。 类A是基类，类B继承类A，类C又继承类B。类A，类B，类C，假设在子类有实现，其对象模型如下图所示。 若菱形继承，两个父类均有对虚函数的实现，而子类没有对虚函数的实现，就会出现二义性问题。一般来说，子类都会进行实现，不会出现这种问题。 经常在父类的析构函数前加virtual，是为了在释放内存时使用子类的析构函数。 纯虚类父类使用纯虚函数virtual void show() = 0;它不会实例化一个对象，在子类继承的时候也会把纯虚函数继承，它强制性要求，派生类必须实现某个接口，不然无法实例化对象","categories":[],"tags":[]},{"title":"智能指针","slug":"智能指针","date":"2023-03-22T12:38:10.000Z","updated":"2023-03-22T12:41:17.019Z","comments":true,"path":"2023/03/22/智能指针/","link":"","permalink":"http://example.com/2023/03/22/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/","excerpt":"","text":"智能指针 c++11std::unique_ptr&lt;T&gt; ：独占资源所有权的指针。当我们独占资源的所有权的时候，可以使用 std::unique_ptr 对资源进行管理——离开 unique_ptr 对象的作用域时，会自动释放资源。 std::unique_ptr 是 move-only 的。 12std::unique_ptr&lt;int&gt; uptr = std::make_unique&lt;int&gt;(200);std::unique_ptr&lt;int&gt; uptr1 = uptr; // 编译错误，std::unique_ptr&lt;T&gt; 是 move-only 的 std::unique_ptr 可以指向一个数组。 可以自定义 deleter。 12345678910&#123; struct FileCloser &#123; void operator()(FILE* fp) const &#123; if (fp != nullptr) &#123; fclose(fp); &#125; &#125; &#125;; std::unique_ptr&lt;FILE, FileCloser&gt; uptr(fopen(&quot;test_file.txt&quot;, &quot;w&quot;));&#125; 123456&#123; std::unique_ptr&lt;FILE, std::function&lt;void(FILE*)&gt;&gt; uptr( fopen(&quot;test_file.txt&quot;, &quot;w&quot;), [](FILE* fp) &#123; fclose(fp); &#125;);&#125; std::shared_ptr&lt;T&gt; ：共享资源所有权的指针。其实就是对资源做引用计数——当引用计数为 0 的时候，自动释放资源。 1234567891011&#123; std::shared_ptr&lt;int&gt; sptr = std::make_shared&lt;int&gt;(200); assert(sptr.use_count() == 1); // 此时引用计数为 1 &#123; std::shared_ptr&lt;int&gt; sptr1 = sptr; assert(sptr.get() == sptr1.get()); assert(sptr.use_count() == 2); // sptr 和 sptr1 共享资源，引用计数为 2 &#125; assert(sptr.use_count() == 1); // sptr1 已经释放&#125;// use_count 为 0 时自动释放内存 也可以指向数组和自定义 deleter。 12345678910111213141516171819&#123; // C++20 才支持 std::make_shared&lt;int[]&gt; // std::shared_ptr&lt;int[]&gt; sptr = std::make_shared&lt;int[]&gt;(100); std::shared_ptr&lt;int[]&gt; sptr(new int[10]); for (int i = 0; i &lt; 10; i++) &#123; sptr[i] = i * i; &#125; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; sptr[i] &lt;&lt; std::endl; &#125; &#125;&#123; std::shared_ptr&lt;FILE&gt; sptr( fopen(&quot;test_file.txt&quot;, &quot;w&quot;), [](FILE* fp) &#123; std::cout &lt;&lt; &quot;close &quot; &lt;&lt; fp &lt;&lt; std::endl; fclose(fp); &#125;);&#125; 一个 shared_ptr 对象的内存开销要比裸指针和无自定义 deleter 的 unique_ptr 对象略大。 shared_ptr 需要维护的信息有两部分： 指向共享资源的指针。 引用计数等共享资源的控制信息——实现上是维护一个指向控制信息的指针。 所以，shared_ptr 对象需要保存两个指针。shared_ptr 的 的 deleter 是保存在控制信息中，所以，是否有自定义 deleter 不影响 shared_ptr 对象的大小。 不能去掉 shared_ptr 对象中指向共享资源的指针。 因为 shared_ptr 对象中的指针指向的对象不一定和控制块中的指针指向的对象一样。（由于多态的存在，有可能指向父类对象）。 12345678910111213141516struct Fruit &#123; int juice;&#125;;struct Vegetable &#123; int fiber;&#125;;struct Tomato : public Fruit, Vegetable &#123; int sauce;&#125;; // 由于继承的存在，shared_ptr 可能指向基类对象std::shared_ptr&lt;Tomato&gt; tomato = std::make_shared&lt;Tomato&gt;();std::shared_ptr&lt;Fruit&gt; fruit = tomato;std::shared_ptr&lt;Vegetable&gt; vegetable = tomato; std::shared_ptr 支持 aliasing constructor。 Aliasing constructor，简单说就是构造出来的 shared_ptr 对象和参数 r 指向同一个控制块（会影响 r 指向的资源的生命周期），但是指向共享资源的指针是参数 ptr。看下面这个例子。 1234567891011using Vec = std::vector&lt;int&gt;;std::shared_ptr&lt;int&gt; GetSPtr() &#123; auto elts = &#123;0, 1, 2, 3, 4&#125;; std::shared_ptr&lt;Vec&gt; pvec = std::make_shared&lt;Vec&gt;(elts); return std::shared_ptr&lt;int&gt;(pvec, &amp;(*pvec)[2]);&#125;std::shared_ptr&lt;int&gt; sptr = GetSPtr();for (int i = -2; i &lt; 3; ++i) &#123; printf(&quot;%d\\n&quot;, sptr.get()[i]);&#125; 使用 std::shared_ptr 时，会涉及两次内存分配：一次分配共享资源对象；一次分配控制块。C++ 标准库提供了 std::make_shared 函数来创建一个 shared_ptr 对象，只需要一次内存分配。 这种情况下，不用通过控制块中的指针，我们也能知道共享资源的位置——这个指针也可以省略掉。 成员函数获取 this 的 shared_ptr 的正确的做法是继承 std::enable_shared_from_this。 123456789101112class Bar : public std::enable_shared_from_this&lt;Bar&gt; &#123; public: std::shared_ptr&lt;Bar&gt; GetSPtr() &#123; return shared_from_this(); &#125;&#125;;auto sptr1 = std::make_shared&lt;Bar&gt;();assert(sptr1.use_count() == 1);auto sptr2 = sptr1-&gt;GetSPtr();assert(sptr1.use_count() == 2);assert(sptr2.use_count() == 2); 一般情况下，继承了 std::enable_shared_from_this 的子类，成员变量中增加了一个指向 this 的 weak_ptr。这个 weak_ptr 在第一次创建 shared_ptr 的时候会被初始化，指向 this。 似乎继承了 std::enable_shared_from_this 的类都被强制必须通过 shared_ptr 进行管理。如果没有创建shared_ptr 直接调用shared_from_this()方法，将会报错。 std::weak_ptr&lt;T&gt; ：共享资源的观察者，需要和 std::shared_ptr 一起使用，不影响资源的生命周期。std::weak_ptr 要与 std::shared_ptr 一起使用。 一个 std::weak_ptr 对象看做是 std::shared_ptr 对象管理的资源的观察者，它不影响共享资源的生命周期： 如果需要使用 weak_ptr 正在观察的资源，可以将 weak_ptr 提升为 shared_ptr。 当 shared_ptr 管理的资源被释放时，weak_ptr 会自动变成 nullptr。 123456789101112131415void Observe(std::weak_ptr&lt;int&gt; wptr) &#123; if (auto sptr = wptr.lock()) &#123; std::cout &lt;&lt; &quot;value: &quot; &lt;&lt; *sptr &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;wptr lock fail&quot; &lt;&lt; std::endl; &#125;&#125;std::weak_ptr&lt;int&gt; wptr;&#123; auto sptr = std::make_shared&lt;int&gt;(111); wptr = sptr; Observe(wptr); // sptr 指向的资源没被释放，wptr 可以成功提升为 shared_ptr&#125;Observe(wptr); // sptr 指向的资源已被释放，wptr 无法提升为 shared_ptr 当 shared_ptr 析构并释放共享资源的时候，只要 weak_ptr 对象还存在，控制块就会保留，weak_ptr 可以通过控制块观察到对象是否存活。","categories":[],"tags":[]},{"title":"c++20","slug":"c-20","date":"2023-03-22T12:35:37.000Z","updated":"2023-03-22T13:09:23.366Z","comments":true,"path":"2023/03/22/c-20/","link":"","permalink":"http://example.com/2023/03/22/c-20/","excerpt":"","text":"语法糖语法糖是指计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是更方便程序员使用。 C++也有很多语法糖，比如运算符重载、lambda表达式、auto类型推导等。这些语法糖可以让我们的代码更简洁、更易读、更高效。例如，下面两种写法是等价的： 12345678int sum = 0;for (int i = 0; i &lt; 10; i++) &#123; sum += i;&#125;int sum = 0;for (auto i : &#123;0,1,2,3,4,5,6,7,8,9&#125;) &#123; sum += i;&#125; c++11、20新特性大多数都是语法糖 c++20C++20有很多新的特性，其中最重要的四个是概念、范围、协程和模块。概念可以让我们定义泛型函数或类的约束条件，范围可以让我们更方便地操作容器和迭代器，协程可以让我们编写异步代码，模块可以让我们更高效地组织代码[。除此之外，C++20还有一些其他的新特性，比如三向比较运算符、指定初始化、日历和时区功能等。 概念概念是一种用来约束模板类型的语法糖。我们可以用concept关键字来定义一个概念，然后用requires关键字来指定一个模板参数必须满足某个概念。例如，我们可以定义一个Integral概念，表示一个类型必须是整数类型 1234567template&lt;typename T&gt;concept Integral = std::is_integral_v&lt;T&gt;;// 然后我们可以用这个概念来约束一个函数模板的参数类型template&lt;Integral T&gt;T add(T a, T b) &#123; return a + b;&#125; 这样，如果我们传入非整数类型的参数，就会在编译时报错。 概念可以自定义，使用requires关键字 12345template&lt;typename T&gt;concept Sortable = requires(T a) &#123; &#123; std::sort(a.begin(), a.end()) &#125; -&gt; std::same_as&lt;void&gt;;&#125;;// 这个概念要求T类型有begin()和end()方法，并且可以用std::sort函数进行排序 标准库中提供了上百种常用的概念，放在和等头文件中。比较常用的一些有：std::same_as, std::derived_from, std::convertible_to, std::floating_point等 12345#include &lt;concepts&gt;template&lt;std::integral T&gt;T add(T a, T b) &#123; return a + b;&#125; 范围范围是C++20加入的一个重要的库功能，它提供了描述范围和对范围的操作的统一接口1。一个范围是可以循环访问的任何东西，比如一个容器或者一个数组2。我们可以用begin()和end()函数来获取一个范围的起始和终止位置3。我们也可以用基于范围的for语句来遍历一个范围中的所有元素。例如，我们可以这样打印一个vector中的所有元素： 12345678#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; v = &#123;1, 2, 3, 4, 5&#125;; for (auto x : v) &#123; std::cout &lt;&lt; x &lt;&lt; &quot; &quot;; &#125;&#125; 自定义的类型，满足range概念，都可以使用范围的特性。即它可以用begin()和end()函数来获取其起始和终止位置。这两个函数返回的对象必须是迭代器或者哨兵。迭代器是可以用++和*操作符来遍历元素的对象，哨兵是可以用&#x3D;&#x3D;操作符来判断是否到达范围的末尾的对象 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;ranges&gt;#include &lt;iostream&gt;class IntRange &#123;public: IntRange(int a, int b) : a_(a), b_(b) &#123;&#125; // 迭代器 class Iterator &#123; public: Iterator(int x) : x_(x) &#123;&#125; int operator*() const &#123; return x_; &#125; Iterator&amp; operator++() &#123; ++x_; return *this; &#125; bool operator==(const Iterator&amp; other) const &#123; return x_ == other.x_; &#125; bool operator!=(const Iterator&amp; other) const &#123; return !(*this == other); &#125; private: int x_; &#125;; // 哨兵 class Sentinel &#123; public: Sentinel(int y) : y_(y) &#123;&#125; bool operator==(const Iterator&amp; iter) const &#123; return *iter == y_; &#125; bool operator!=(const Iterator&amp; iter) const &#123; return !(*this == iter); &#125; private: int y_; &#125;; // begin()和end()函数 Iterator begin() const &#123; return Iterator(a_); &#125; Sentinel end() const &#123; return Sentinel(b_); &#125;private: int a_, b_;&#125;;int main() &#123; IntRange r(1,5); for (auto x : r) &#123; std::cout &lt;&lt; x &lt;&lt; &quot; &quot;; &#125;&#125; 协程协程是一种可以在执行过程中被挂起和恢复的函数。它可以用来实现异步编程，提高性能和并发度。 C++20中引入了三个新的关键字，co_await，co_yield和co_return，用来标记一个函数是协程。这些关键字只是语法糖，编译器会将协程的上下文打包成一个对象，并让未执行完的协程先返回给调用者。要实现一个C++20协程，还需要提供两个鸭子类型，promise type和awaiter type，分别用来管理协程的生命周期和等待机制。 例如，我们可以实现一个简单的生成器协程，它每次产生一个整数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;coroutine&gt;#include &lt;iostream&gt;// promise typestruct Generator &#123; struct promise_type &#123; int current_value; std::suspend_always yield_value(int value) &#123; this-&gt;current_value = value; return &#123;&#125;; &#125; std::suspend_always initial_suspend() &#123; return &#123;&#125;; &#125; std::suspend_always final_suspend() noexcept &#123; return &#123;&#125;; &#125; Generator get_return_object() &#123; return Generator&#123;std::coroutine_handle&lt;promise_type&gt;::from_promise(*this)&#125;; &#125; void unhandled_exception() &#123;&#125; &#125;; // awaiter type bool move_next() &#123; p.resume(); return !p.done(); &#125; int current_value() &#123; return p.promise().current_value; &#125;private: std::coroutine_handle&lt;promise_type&gt; p;&#125;;// 协程函数Generator generator(int start = 0) &#123; int i = start; while (true) &#123; co_yield i++; &#125;&#125;int main() &#123; auto g = generator(1); for (int i = 0; i &lt; 10; ++i) &#123; g.move_next(); std::cout &lt;&lt; g.current_value() &lt;&lt; &quot; &quot;; &#125;&#125; 使用协程实现异步网络编程的主要优点是可以用同步的语法写出异步的代码，提高代码的可读性和可维护性1。要使用协程实现异步网络编程，需要以下几个步骤： 使用标准库中提供的std::jthread或std::thread创建一个或多个工作线程，用来执行协程任务。 使用标准库中提供的std::coroutine_handle或自定义的协程句柄类型，管理协程的生命周期和调度。 使用标准库中提供的std::future或自定义的awaiter类型，等待异步操作完成并获取结果。 使用标准库中提供的std::sync_wait或自定义的同步等待函数，等待所有协程任务完成后退出程序。 例如，我们可以使用一个简单的网络框架ZED3，它提供了一些基本的异步IO操作，并封装了协程句柄和awaiter类型。我们可以用以下代码实现一个简单的回显服务器： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;zed/net.hpp&gt;#include &lt;iostream&gt;using namespace zed;int main() &#123; // 创建一个io_context对象 io_context ctx; // 创建一个工作线程 std::jthread th([&amp;ctx]() &#123; ctx.run(); &#125;); // 创建一个tcp服务器 tcp_server server(ctx); // 绑定端口 server.bind(8080); // 开始监听 server.listen(); while (true) &#123; try &#123; // 接受连接，并返回一个tcp_socket对象 auto socket = co_await server.accept(); std::cout &lt;&lt; &quot;New connection from &quot; &lt;&lt; socket.remote_endpoint() &lt;&lt; &quot;\\n&quot;; while (true) &#123; // 接收数据，并返回接收到的字节数 auto n = co_await socket.recv(); if (n == 0) break; // 连接断开 std::cout &lt;&lt; &quot;Received &quot; &lt;&lt; n &lt;&lt; &quot; bytes\\n&quot;; // 发送数据，并返回发送出去的字节数 auto m = co_await socket.send(n); std::cout &lt;&lt; &quot;Sent &quot; &lt;&lt; m &lt;&lt; &quot; bytes\\n&quot;; &#125; std::cout &lt;&lt; &quot;Connection closed\\n&quot;; &#125; catch (const std::exception&amp; e) &#123; std::cerr &lt;&lt; e.what() &lt;&lt; &quot;\\n&quot;; &#125; &#125;&#125; 模块C++20模块是一种新的代码组织和重用的方式，它可以替代传统的头文件和翻译单元。模块可以提高编译速度，避免宏污染，隐藏实现细节，简化依赖关系等优点。要使用模块，需要以下几个步骤： 在源文件中使用module关键字声明一个模块，并指定模块名。 在源文件中使用export关键字导出需要对外提供的符号。 在其他源文件中使用import关键字导入需要使用的模块。 使用支持模块的编译器编译源文件，并生成相应的模块接口文件和目标文件。 例如，我们可以用以下代码定义一个名为hello的模块： 123456// hello.cppmmodule hello; // 声明一个名为hello的模块export void say_hello(); // 导出一个名为say_hello的函数void say_hello() &#123; std::cout &lt;&lt; &quot;Hello, world!\\n&quot;;&#125; 然后我们可以在另一个源文件中导入并使用这个模块： 12345// main.cppimport hello; // 导入hello模块int main() &#123; say_hello(); // 调用say_hello函数&#125; 子模块是一种在逻辑上划分模块的方法，它可以让用户选择性地导入模块的一部分或全部内容。子模块的命名规则中允许点存在于模块名字当中，但点并不代表语法上的从属关系，而只是帮助程序员理解模块间的逻辑关系。 例如，我们可以用以下代码定义一个名为hello.sub_a的子模块： 123456// hello.sub_a.cppmexport module hello.sub_a; // 声明一个名为hello.sub_a的子模块export void say_hello_sub_a(); // 导出一个名为say_hello_sub_a的函数void say_hello_sub_a() &#123; std::cout &lt;&lt; &quot;Hello, sub a!\\n&quot;;&#125; 然后我们可以在另一个源文件中定义一个名为hello.sub_b的子模块： 123456// hello.sub_b.cppmexport module hello.sub_b; // 声明一个名为hello.sub_b的子模块export void say_hello_sub_b(); // 导出一个名为say_hello_sub_b的函数void say_hello_sub_b() &#123; std::cout &lt;&lt; &quot;Hello, sub b!\\n&quot;;&#125; 最后我们可以在另一个源文件中定义一个名为hello的父模块，它导出了两个子模块： 1234// hello.cppmexport module hello; // 声明一个名为hello的父模块export import hello.sub_a; // 导出并导入hello.sub_a子模块export import hello.sub_b; // 导出并导入hello.sub_b子模块 这样，用户就可以根据需要导入不同的子模块或父模块： 123456// main.cppimport hello; // 导入hello父模块，相当于同时导入了两个子模块int main() &#123; say_hello_sub_a(); // 调用say_hello_sub_a函数 say_hello_sub_b(); // 调用say_hello_sub_b函数&#125; 命名空间冲突是指不同的模块或源文件中定义了相同的名称，导致编译器无法区分它们的含义。C++20 模块提供了一些方法来避免或解决命名空间冲突： 使用不同的模块名字来区分不同的模块，例如 hello.sub_a 和 hello.sub_b 就是两个不同的模块，即使它们都定义了 say_hello 函数，也不会发生冲突。 使用限定名字来指定模块中的名称，例如 hello.sub_a::say_hello 和 hello.sub_b::say_hello 就可以明确地区分两个模块中的函数。 使用 using 声明或 using 指令来引入需要的名称，但要注意避免引入重复或冲突的名称。例如: 1234567// main.cppimport hello; // 导入hello父模块using hello.sub_a::say_hello; // 引入hello.sub_a中的say_hello函数int main() &#123; say_hello(); // 调用hello.sub_a中的say_hello函数 hello.sub_b::say_hello(); // 调用hello.sub_b中的say_hello函数&#125; 使用 export 关键字来控制哪些名称被导出到其他模块或源文件，以减少暴露给外部的名称。例如： 123456789101112// math.cppmexport module math; // 声明一个名为math的模块namespace detail &#123; // 定义一个未导出的命名空间detail int add(int x, int y) &#123; return x + y; &#125; // 定义一个未导出的函数add&#125;export int sum(int x, int y) &#123; return detail::add(x, y); &#125; // 定义并导出一个函数sum，它调用了detail命名空间中的add函数// main.cppimport math; // 导入math模块int main() &#123; int s = math::sum(1, 2); // 调用math模块中导出的sum函数 int a = math::detail::add(1, 2); // 错误：math模块没有导出detail命名空间或add函数","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"http://example.com/tags/c/"}]},{"title":"引用","slug":"引用","date":"2023-03-22T11:39:05.000Z","updated":"2023-03-22T12:37:53.941Z","comments":true,"path":"2023/03/22/引用/","link":"","permalink":"http://example.com/2023/03/22/%E5%BC%95%E7%94%A8/","excerpt":"","text":"指针与引用引用必须要初始化。 指针会根据编译器不同而变化，32位4字节，64位8字节 引用根据被引用的数据类型变化 int*&amp; 指针的引用 以指针来判断。 右值引用 c++11 左值：可以长时间保存，可以存在于&#x3D;左边的值，可以取地址； 右值：临时值，不能存在于&#x3D;左边的值，不可以取地址。 左值引用，实际上是取地址赋给新的变量。必须初始化。 常引用，用于引用部分右值，不可进行更改。实际上是使用一个临时变量与一块临时内存进行存储，必须初始化。可以引用左与右。 右值引用原理相近，临时内存的地址无法获取，但是可以对临时内存里面的内容进行修改。 12int&amp;&amp; v1 = 10;v1++; 右值引用是C++11新特性，之所以引入右值引用，是为了提高效率。如下面所示： 1234567891011121314151617181920212223242526272829303132333435363738394041class A&#123;public: A(size_t N):m_p(new char[N]) &#123; &#125; A(const A &amp; a) &#123; if (this != &amp;a) &#123; delete[]m_p; m_p = new char[strlen(m_p) + 1]; memcpy(m_p, a.m_p, strlen(m_p) + 1); &#125; &#125; ~A() &#123; delete []m_p; &#125; private: char *m_p = nullptr;&#125;; A createA(size_t N)&#123; return A(100);&#125; void func(A a)&#123; //&#125; int main()&#123; func(createA(100)); system(&quot;pause&quot;); return 0;&#125; 这里会导致大量得调用A得构造函数，不考虑编译优化，原本执行如下： 123456createA(100)，执行A(100)调用A(size_t)构造函数一次；退出createA，临时构造得A(100)，释放调用析构函数一次；赋给返回值会调用一次拷贝构造函数一次；返回值传入func中形参会调用拷贝构造函数一次；func运行完成后形参释放，调用A析构函数一次；返回值使用完成释放，调用A析构函数一次； 从上面可以看出有大量得构造、析构调用 ，但是我们做的工作无非就是临时构造一个A(100)给func使用而已。那么可否将临时A(100)始终一份给到func使用呢？答案就是右值引用。如下： 1234567891011121314151617181920212223242526272829303132class A&#123;public: A(size_t N):m_p(new char[N]) &#123; &#125; ~A() &#123; delete []m_p; &#125; private: char *m_p = nullptr;&#125;; A&amp;&amp; createA(size_t N)&#123; return (A&amp;&amp;)A(100);&#125; void func(A&amp;&amp; a)&#123; //&#125; int main()&#123; func(createA(100)); system(&quot;pause&quot;); return 0;&#125; 我们将临时A(100)强制转换为了右值引用，同时func形参也是右值引用，也就是将临时对象延长到了func中，中间避免了其他构造和析构调用，提高了效率。 ​ 注意到我们将A得拷贝构造函数去掉了，因为已经用不到。如果原版写法，去掉拷贝构造函数会崩溃，因为会自动调用默认拷贝构造函数，是浅拷贝，中间临时对象会提前删除公共内存，后面对象再次释放是就会重复删除内存导致崩溃。 这就是移动。它可以让你将一个对象的资源（如内存、文件句柄等）从一个临时的右值转移给另一个对象，而不需要进行深拷贝这样可以提高性能，避免不必要的内存分配和释放 std::move可以转换左值引用为右值引用。实现原理实际上就是强制转换 123456789int main()&#123; int a = 3; int &amp;&amp;t = std::move(a); int &amp;&amp;t2 = std::move(3); system(&quot;pause&quot;); return 0;&#125; 12345678910int main()&#123; int a = 3; int &amp;&amp;t = (int &amp;&amp;)a; t = 9; cout &lt;&lt; a &lt;&lt; endl; // a = 9 system(&quot;pause&quot;); return 0;&#125; std::unique_ptr不能相等，因为他们是不可以拷贝的，因此不可以左值赋给左值。使用移动，把左值转换成右值，就可以让二者相等。 通用引用通用引用就是根据接受值类型可以自行推导是左值引用还是右值引用。 如果声明变量或参数具有T&amp;&amp;某种推导类型的类型 T，则该变量或参数为通用引用，否则就是右值引用（无法传入左值）。 也就是传入的参数在编译时需要推导，如果不需要推导，则不是通用引用。如下： 123456789101112131415template&lt;typename T&gt;class B&#123;public: void print(T &amp;&amp;) &#123;&#125;&#125;; int main()&#123; B&lt;int&gt; b; b.print(3); // 为右值引用 system(&quot;pause&quot;); return 0;&#125; 因为在编译print之前print中的参数已经由B b确定了，所以在print编译时无需推导，故B中的T&amp;&amp;为右值引用。如果改为如下： 12345678910111213141516template&lt;typename T&gt;class B&#123;public: template&lt;typename Arg&gt; void print(Arg &amp;&amp;) &#123;&#125;&#125;; int main()&#123; B&lt;int&gt; b; b.print(3); // 为右值引用 system(&quot;pause&quot;); return 0;&#125; 因为print时函数模板形参和类模板形参类型时独立的，故在编译print时是需要推导的，故Arg&amp;&amp;为通用引用。 引用折叠引用虽然形式上是右值引用，但是却可以接受左值，这是怎么实现的呢？这就是引用折叠。 12345678910111213template&lt;typename T&gt;void print(T&amp;&amp; t) &#123;&#125; int main()&#123; int a = 9; print(a); print(9); system(&quot;pause&quot;); return 0;&#125; print(a)时，因为a为左值,会被推导成print(int&amp; &amp;&amp;t)形式，int&amp; &amp;&amp;t 会被折叠为int &amp;，所以最终形式为print(int &amp;)。（左值被推导为左值引用） print(9)时，为9为右值，所以被推导为print(int&amp;&amp; &amp;&amp;)形式，而int&amp;&amp; &amp;&amp;会被折叠为int&amp;&amp;，所以最终形式为print(int&amp;&amp;)。（右值被推导为右值引用） 引用类型只有两种，所以折叠形式就是4中，为：T&amp; &amp;,T&amp; &amp;&amp;,T&amp;&amp; &amp;,T&amp;&amp; &amp;&amp;。引用折叠规则概况为两种： T&amp;&amp; &amp;&amp;折叠为T&amp;&amp;; 其他折叠为T&amp;. 完美转发通用引用既可以接受左值也可以接受右值，但是通用引用本身是左值。如果在函数模板中继续传递该值给其他函数，势必会改变该值的属性，即都为左值引用。 使用std::forward(a)可以进行完美转发，使值属性和之前保持一致。某个功能对左值和右值处理情况不一致，如果将左值和右值引用当作同一种情况使用，可能会会有性能损失。例如左值进行深拷贝，右值进行移动。 原理是使用了引用折叠。具有推导类型的T&amp;&amp;转换会进行引用折叠。而int&amp;&amp;类型是确定的，不能进行折叠。 有两套，传入的为左或右，用右值进行强制类型转换，左右转化为左，右右转化为右","categories":[],"tags":[]},{"title":"hexo快速入门","slug":"hexo快速入门","date":"2023-03-20T03:34:10.000Z","updated":"2023-03-20T05:31:55.202Z","comments":true,"path":"2023/03/20/hexo快速入门/","link":"","permalink":"http://example.com/2023/03/20/hexo%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","excerpt":"安装安装node.js安装git安装hexo","text":"安装安装node.js安装git安装hexo 新开一个文件夹，右键，在这里打开git bash 12npm install -g hexo-clinpm install --save hexo-deployer-git 配置github SSHssh可以免密的将本地的源码和资源上传到github，无需要每次都输账号和密码。 12cd ~ssh-keygen -t rsa -C &quot;邮件地址&quot; 在用户文件夹下生成了一个.ssh文件夹，进入，复制.pub文件中全部内容 打开github主页，点击个人设置，点击左侧的SSH and GPG keys，点击New SSH key 将id_rsa.pub复制的内容粘贴到key中，title随便起一个就行。 配置好用户名和邮箱 12git config --global user.name &quot;xxx&quot; #你的github用户名git config --global user.email &quot;xxx@163.com&quot; #填写你的github注册邮箱 写个人博客初始化12hexo initnpm install 新建一个博客1hexo n &lt;title&gt; 存放地址：source/_posts 解决图片问题_config.yml文件做如下修改：yaml post_asset_folder: true 创建同名文件夹，可以放入图片 使用的是Typora编辑器，可以在编辑器的文件&#x2F;偏好设置&#x2F;图像中进行如下设置： 复制到指定路径./$(filename)/ 安装图片显示插件 1npm install hexo-asset-image 生成本地静态网站并预览123hexo cleanhexo ghexo s 截断在文章中加入&lt;!--more--&gt;或在开头加入description: xxx 部署到github新建一个库，名字叫 username.github.io 将本地库与远端github仓库绑定123456git initgit add .git commit -m &quot;commit&quot;git remote add origin 仓库的http地址git pull --rebase origin maingit push [-u] origin main 编辑_config.yml1234deploy: type: git repository: 仓库的ssh地址 branch: main 在bash部署123hexo cleanhexo ghexo d 更改主题安装依赖1npm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive 下载对应的主题1git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly 更改配置文件编辑_config.yml 1theme: butterfly 主题文档1https://butterfly.js.org/posts/21cfbf15/ 开启live2d安装依赖1npm install --save hexo-helper-live2d 下载model文件模型浏览live2d-widget-model-chitoselive2d-widget-model-epsilon2_1live2d-widget-model-gflive2d-widget-model-haru_01live2d-widget-model-haru_02live2d-widget-model-harutolive2d-widget-model-hibikilive2d-widget-model-hijikilive2d-widget-model-izumilive2d-widget-model-koharulive2d-widget-model-mikulive2d-widget-model-nicolive2d-widget-model-nietzschelive2d-widget-model-ni-jlive2d-widget-model-nipsilonlive2d-widget-model-nitolive2d-widget-model-shizukulive2d-widget-model-tororolive2d-widget-model-tsumikilive2d-widget-model-unitychanlive2d-widget-model-wankolive2d-widget-model-z16 1https://blog.csdn.net/wang_123_zy/article/details/87181892 模型下载1npm install live2d-widget-model-shizuku 配置配置Hexo的主_config.yml或者使用的主题的_config.yml 添加以下代码到配置文件中： 12345678910111213141516## Live2D看板娘live2d: enable: true pluginModelPath: assets/ model: #模板目录，在node_modules里 use: live2d-widget-model-shizuku display: position: right width: 300 height: 600 mobile: # 在手机端显示 show: false rect: opacity:0.7","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"c++","slug":"c","permalink":"http://example.com/tags/c/"}]}